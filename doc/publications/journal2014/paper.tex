%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\def\includegraphic{}
\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs

\newcommand{\variantSpark}{{\sc VariantSpark}}
\newcommand{\ARI}{Adjusted Rand Index}


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{VariantSpark: Population Scale Clustering of Genotype Information}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1,aff2},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={aff1},                       % id of corresponding address, if any
%   noteref={n1},                        % id's of article notes, if any
%   email={jane.e.doe@cambridge.co.uk}   % email address
]{\inits{}\fnm{Aidan R.} \snm{O'Brien}}
\author[
   addressref={aff1},
]{\inits{}\fnm{Neil} \snm{Saunders}}
\author[
   addressref={aff3,aff4},
%   email={john.RS.Smith@cambridge.co.uk}
]{\inits{}\fnm{Fabian A.} \snm{Buske}}
\author[
   addressref={aff1},
   email={Denis.Bauer@CSIRO.au}
%   email={john.RS.Smith@cambridge.co.uk}
]{\inits{}\fnm{Denis C.} \snm{Bauer}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{CSIRO, Health \& Biosecurity Flagship}, % university, etc
  \street{11 Julius Av},                     %
  \postcode{2113},                                % post or zip code
  \city{Sydney},                              % city
  \cny{Australia}                                    % country
}
\address[id=aff2]{%
  \orgname{School of Biomedical Sciences and Pharmacy, Faculty of Health},
  \postcode{2308},
  \city{Newcastle},
  \cny{Australia}
}
\address[id=aff3]{%
  \orgname{Cancer Epigenetics Program, Cancer Research Division, Kinghorn Cancer Centre, Garvan Institute of Medical Research},
  \street{384 Victoria St},
  \postcode{2010},
  \city{Sydney},
  \cny{Australia}
}
\address[id=aff4]{%
  \orgname{UNSW Medicine, University of New South Wales},
%  \street{384 Victoria St},
  \postcode{2052},
  \city{Sydney},
  \cny{Australia}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
\parttitle{Motivation} Genomic information is increasingly used in medical practice, giving rise to the need for efficient analysis methodology able to cope with thousands of individuals and millions of variants. The widely used Hadoop MapReduce architecture and associated machine-learning library, Mahout, provide the means for tackling computationally challenging tasks. However, many genomic analyses do not fit the Map-Reduce paradigm. We therefore survey the recently developed {\sc Spark} engine, along with its associated machine learning library, MLlib, which are more flexible in the parallelisation architecture and hence more suitable for population-scale bioinformatics tasks. To do this, we developed an interface from Mahout and MLlib to the standard variant format (VCF), which opens up the usage of advanced, efficient machine learning algorithms to genomic data. 

\parttitle{Results} We successfully clustered more than 2,500 individuals each having more than 38 Million variants. Both Hadoop and {\sc Spark} have superior performance over traditional approaches. We observe a 50 fold speedup when using the efficient in-memory {\sc Spark} compute engine compared to Hadoop. Furthermore, our implementation achieves a better performance compared to a previously published {\sc Spark}-based genome clustering approach, ADAM.

\parttitle{Availability} The package is written in Scala and available at \url{https://github.com/BauerLab/VariantSpark}.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{sample}
\kwd{article}
\kwd{author}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Background}

Genomic information is increasingly used in medical practice.
A commonly performed task in such applications is grouping individuals based on their genomic profile to identify population association~\cite{Gao2007} or elucidate different haplotype involvement in diseases susceptibility~\cite{Laitman2013}.  
Due to the decreasing sequencing cost it is economical to generate studies with sample sizes previously reserved for larger consortia such as the 1000 genomes project~\cite{1KG2012} or The Cancer Genome Atlas, TCGA~\cite{TCGA2013}. 
At the same time, whole genome sequencing enables the inclusion of rare or even somatic mutations in the analysis, increasing the feature space by orders of magnitude. This drastic increase in both sample numbers and features per sample requires a massively parallel approach to data processing~\cite{Stein2010}. Traditional parallelisation strategies like OpenMPI or hardware accelerators (GPGPU) cannot scale to variable data sizes or require purpose-build hardware.
 
Addressing this issue, {\sc Apache Hadoop MapReduce}~\cite{Borthakur2007} transforms data into 'key-value pairs' that can then be distributed between multiple nodes across a commodity compute-cluster depending on the size of the problem. 
MapReduce approaches are increasingly being used in bioinformatics (for reviews see~\cite{Zou2013, Qiu2010,Taylor2010}). 
This is especially the case for sequence analysis tasks, such as read mapping~\cite{Schatz2009}, duplicate removal~\cite{Jourdren2012}, and variant calling~\cite{Langmead2009, McKenna2010} as well as Genome Wide Analysis Study based tasks~\cite{Huang2013, Guo2014}. 
Apache also developed a machine learning library, Mahout~\cite{Owen2011}, which allows efficient out-of-the-box analysis to be applied to clinical applications, such as medical health records~\cite{Ko2014}.
Unfortunately, the MapReduce paradigm is not always the optimal solution, specifically for bioinformatics or machine learning applications that require iterative in-memory computation. In addition, Hadoop is disk IO intensive, and this can prove to be a bottleneck in processing-speed.

{\sc Apache Spark}~\cite{Zaharia2011} is a more recent compute engine, which overcomes many of Hadoop's limitations. 
One of the main benefits is that it allows programs to cache data in memory; potentially eliminating, or at least reducing, the bottleneck of disk IO. 
When utilising caching, Apache claim {\sc Spark} to be 100x faster than Hadoop. 
Although {\sc Spark} allows MapReduce-like programs, it does not require programs to exactly model the MapReduce paradigm, which in turn allows more flexibility when designing programs. 
Recognising the capability, the Big Data Genomics (BDG) group has recently demonstrated the strength of {\sc Spark} in a genomic clustering application using ADAM, a set of formats and APIs as well as processing stage implementations for genomic data~\cite{Massie2013}. 
While the speedup over traditional methods was impressive, being limited by constraints within this general genomics framework hampered performance. 

We hence developed a more focused purpose-built application in {\sc Spark} to perform genomic clustering of individuals. 
We utilise {\sc Spark}'s machine-learning library, MLlib, and provide an interface from the standard variant data format, variant call format (VCF)~\cite{1KG2012}, which opens up the application of MLlib's different machine learning algorithms a wide range of genotype-based analysis tasks. 
To demonstrate its capability, we cluster variant datasets from the 1000 genomes project~\cite{1KG2012} to determine population structure using the k-means clustering algorithm available in MLlib's. 
In the first section we benchmark the performance and accuracy of MLlib's implementations against Hadoop's Mahout as well as more traditional methods (R, Python) limiting the used data to chromosome 22.
In section two we demonstrate {\sc Spark}'s full capacity by seamlessly scaling from 1\% to 100\% of the human genome.
In section three we replicate the analysis by using XX genomes from the Personal Genome Project~\cite{Lunshof2013}.
In the last section we discuss the pipeline for visualising the resulting cluster. 


\section*{Results and discussions}

\subsection*{{\sc Spark} enables faster clustering of individuals compared to traditional methods}

In this section we compare the time required to cluster individuals based on genomic variants using \variantSpark{} against ADAM as well as the more traditional approaches using Hadoop Mahout, Python and R. 
We limit the genomic variants to only chromosome 22 as the traditional approaches have substantially larger memory consumption rendering a whole-genome input infeasible.  
Furthermore, we perform the comparisons on a single virtual-machine to ensure the five different approaches have access to the same resources.
We use k-means clustering algorithm in the respective implementations, which all require the VCF input files to be pre-processed (see methods). 
We therefore compare the time required for the pre-processing step, as well as the time required for k-means clustering.

As shown in Table~\ref{fivewaycomparison}, pre-processing the data is fastest in \variantSpark{}, requiring 2mins and 58secs. %178s 
This is almost 80\% faster than the ADAM implementation (12min and 48secs) %768s
or our Hadoop implementation(14min and 22secs). %862s
Unlike \variantSpark{} or our Hadoop implementation, which pre-process VCF files directly, ADAM requires the input to be in their ADAM file format. 
VCF files must hence be converted to this binary format before they can be pre-processed into the row vector format. 
Although this additional pre-processing step is only required as a one-off for each input file, it requires an additional 13mins and 13secs %793s
rendering our approach almost an order of magnitude faster. 
R on the other hand takes 67 minutes and Python 92 minutes.
This substantially different runtime for the pre-processing is due to the Python and R implementations not natively supporting multithreading while \variantSpark{}, ADAM and Hadoop using 8 CPUs.

Clustering the samples 30\% fastest in \variantSpark{} compared to ADAM (1min and 20secs compared to 1min and 52secs) and  %80 vs 112
90\% faster than Hadoop (14mins and 23secs), see Table~\ref{fivewaycomparison}.
The similarity between \variantSpark{} and ADAM is expected, as they both use {\sc Spark}'s MLlib k-means implementation. 
The 30 second speed increase in \variantSpark{} is likely due to us converting the VCF files to sparse vectors, whereas ADAM creates dense vectors which are less memory efficient.

To demonstrate {\sc Spark}'s advantages over parallel approaches to k-means clustering in existing libraries, we compare the clustering libraries used in {\variantSpark}, to those in Python and R. 
In regards to speed, clustering using Python and R were both an order of magnitude slower than \variantSpark{} and ADAM, however both were faster than our Hadoop implementation, where Python required 11min and 29sec and R required 7min and 25sec. 
This is due the dataset (chromosome 22) being small enough to fit into memory on a single machine. This enables Python and R to store the output from each k-means iteration directly in memory. Hadoop, however, is designed to write the entire output of each iteration to disk.
Although this feature allows for greater scalability, as memory availability is less of an issue, the bottleneck introduced from disk IO increases processing time notably.
This demonstrates one of the major improvements of {\sc Spark} over Hadoop, where {\sc Spark} can cache datasets in memory. This virtually eliminates the disk IO bottleneck on smaller datasets, or at least reduces it on larger datasets where only a portion of the data can be cached in memory.

For smaller datasets, we've demonstrated {\sc Spark} to be faster than it's single-machine counterparts, however it's also important that this speedup isn't at the cost of quality.
The 1000 Genome data is ideal for investigating the clustering quality, as the consortium provides metadata which lists the super-population that each individual belongs to. This knowledge allows us to create a `truth' clustering, where individuals are clustered by their super-population.
With the four super-populations (AMR, AFR, EAS, SAS), we can divide the individuals up into four clusters based on these labels. We can then compare this truth clustering with the k-means clusters.
For this comparison, we use the Adjusted Rand Index (ARI) metric. To generate the ARI, an algorithm takes two separate clusterings and returns a value based on their similarity.
Two identical clusterings will result in a 1, whereas 0 represents two clusterings that do not agree on any points (individuals). On Chromosome 22, each or the algorithms resulted in an ARI of 0.84. Although not perfect, this demonstrates that the speed-up in spark isn't at the cost of quality, when compared to R and Python.
A result of 1 is probably unlikely because, as mentioned by 10.15252/embr.201439469, although differences in the distribution of alleles exist between populations, the boundaries are rather diffuse. %TODO - proper citation


\subsection*{\variantSpark{} allows genome wide sampling of variants to improve clustering quality}

Although we've demonstrated the advantages of \variantSpark{} with smaller datasets, the main advantage of {\sc Spark} (and hense \variantSpark) is it's scalability and ability to process datasets that exceed memory limits.
To demonstrate the scalability, we continue to cluster the same individuals from the 1000 Genome Project, however, rather than clustering on a single chromosome, we cluster the individuals on different subsets of variants from their 22 autosomes, and finally every variant.
We run this comparison on our in-house Hadoop cluster. Pre-processing the VCF files takes approximately 12 minutes, 19 minutes and 40 minutes for 20\%, 40\% and 100\% of the genome, respectively.
In each case, the memory required doesn't exceed a modest 2GB per executor, even for 100\% of the genome. Memory requirements for clustering, however, increase in correlation with the dataset. 

Where our previous clustering of chromosome 22 required no more than 4GB memory per executor, scaling to 100\% of the genome increases the requirement to more than 24GB. This scales in an approximate linear trend, where 20\% of the genome requires 6GB and 40\% of the genome requires 12GB.
24GB per executor for k-means clustering makes it much more resource intensive than pre-processing's 2GB per executor. 
One factor which results in these differing memory requirements is the sparsity of the vectors used to store the variants. Due to their sparse nature (the abundance of 0s) we can store them efficiently as Sparse Vectors in our pre-processing stage. However, to cluster these vectors, the k-means algorithm converts these sparse vectors to dense vectors. This is much less memory-efficient, as dense vectors explicitly define zero values (which are excluded from Sparse Vectors so don't require any memory), considerably increasing the memory required.
So although we demonstrate the potential of k-means clustering to scale to 100\% of the genome, machine learning algorithms that can deal with sparse vectors would potentially be able to scale much further, or for example, easily deal with more alleles.


\subsection*{Personal Genome Project}
To demonstrate the versatility of \variantSpark{} we also process and cluster individuals from the Personal Genomics Project. Unlike the 1000 genomes project, this data is open 


\subsection*{Graph visualization}
Add the image here \ldots



\section*{Conclusions}
\variantSpark{} 
To summarise, \variantSpark{} completes the whole variant clustering task in just over 4 minutes, which is 80\% faster than ADAM (28 mins), 85\% faster than Hadoop (29 min), 94\% faster than R (75 min), and 95\% faster than Python (103 min).



\section*{Materials and methods}
\subsection*{Computational resources}
We completed the Chromosome 22 comparisons on a virtual machine (VM) hosted on Microsoft Azure. This VM is an A7 Linux instance with 8 cores, 56GB memory and Ubuntu as the OS. 
For the whole-genome clustering, we used our in-house Hadoop cluster. This uses Hadoop 2.5.0, managed by CDH 5. We use Spark 1.3.1. This 13 node cluster has a total of 416 cores and 1.22TB memory.


\subsection*{Datasets}
We used  phase 1 variants from the 1000 Genomes Project. On the 22 autosomes, this dataset contains variants for 1092 individuals, spread over a total 38,219,238 alleles.
We use chromosome 22 for some comparisons, and this contains variants over 494,328 alleles.
These individuals from these datasets are distributed across four super populations, African (AFR), Mixed American (AMR), East Asian (EAS) and European (EUR).


\subsection*{VariantSpark Implementation}
For each of the implementations, we pre-process the VCF files to arrays of features.
In \variantSpark{} we read in the VCF files as text files to a Resilient Data Set (RDD). RDDs allow us to process the files, line-by-line, in parallel. We parse each line as tab-separated values, and `zip' each value with it's heading from the VCF file.
The key-value pairs (KVPs) from each line in the VCF file are stored in separate arrays. We then `zip' each array with a unique index, where these indexes form identifiers for the alleles.
At this point, we have one array for each line from the VCF file. The values in the arrays are KVPs of the parsed tab-separated values zipped with their heading (individual ID), and each array has a unique index.
We now use a `flatMap' to convert these indexed arrays of KVPs into a less convoluted data-structure, as well as to filter out KVPs that aren't alleles (i.e. KVPs that are VCF metadata).
This results in KVPs where the key is the individual ID, and the value is a tuple of the allele and it's index. We effectively have one KVP for each allele from the VCF file. At this point, each allele is still a string straight from the VCF file (i.e. `0\textbar1'), however, we require doubles for MLLib.
To convert this to a double, we take the Hamming distance of the allele, where a 1 represents a heterozygous variant, 2 a homozygous variant, and 0, no variant. Also, for each individual, we filter out any alleles that have no variant, leaving us with KVPs of variants (homozygous and heterozygous) only.
We can now group these KVPs by their key (the individual ID) and map them to sparse vectors. This results in a sparse vector for each individual, the format required by MLLibs machine learning algorithms.


\subsection*{ADAM Implementation}
For our ADAM comparison, we followed the ADAM implementation from (\url{http://bdgenomics.org/blog/2015/02/02/scalable-genomes-clustering-with-adam-and-spark/}).


\subsection*{Hadoop Implementation}
We use our previous Hadoop implementation, which is based on the MapReduce model. This makes use of KVPs, similarly to \variantSpark{}, however, it is explicitly based on the MapReduce model.
Because of this, and because we need a unique range of identifiers for alleles (in the smallest range possible), we need to run an initial MapReduce task to index the lines. This is comparable (however, more cumbersome) to the `zip' operation in \variantSpark{}.
The second MapReduce task does the bulk of the work. The Map stage begins by creating KVPs from the VCF file. For each KVP, the key is a tuple of a primary and secondary key, where the primary key is the individual ID and the secondary key is the allele ID. The value for each KVP is the variant.
The primary key ensures that KVPs for each individual are distributed to the same node during the MapReduce shuffle stage. After being distributed, the KVPs for each individual are sorted by their secondary key.
Now that the KVPs for each individual are physically located on the same hardware, the Reduce stage can efficiently create a sparse vector for each individual from these KVPs. The sparse vectors (one for each individual) are saved to disk, ready to be used as the input to one of MLLibs algorithms.


\subsection*{R Implementation}
For our R implementation, we read in a VCF file as a table and convert it to a matrix, dropping the columns that don't represent alleles. As with our \variantSpark{} pre-processing, we convert the strings that represent each allele to a numeric value.
We then transpose the matrix, resulting in a data-structure where each row represents an individual.


\subsection*{Python Implementation}
Our Python implementation reads in lines from a VCF file as tab-separated values and stores the lines in a pandas DataFrame. The column headings are the individual IDs and the row headings are the allele locations.
We drop the first columns that aren't alleles, and then convert the remaining allele strings to numeric values. We can then transpose the DataFrame. Although to cluster this DataFrame (using sci-kit learn), we simply need to convert it to a matrix with {\sc .as\textunderscore{}matrix()}


%% TODO - NEIL - (and better heading) %%
\subsection*{Personal Genome Project VCF making}
The PGP data..... 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
  The authors declare that they have no competing interests.

\section*{Author's contributions}
    Text for this section \ldots

\section*{Acknowledgements}
  A.R.O was funded by the NSW Cancer Institute Big Data Big Impact schema, F.A.B by the National Health and Medical Research Council [1051757] and both D.C.B and N.S by Commonwealth Scientific and Industrial Research Organisation's Transformational Capability Platform, Science and Industry Endowment Fund and Information Management and Technology Services. The computation on Azure was funded by Microsoft Azure Research Award. 
The authors would like to thank Piotr Szul and Gareth Williams for their help with setting up Hadoop on the HPC system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{genotypeClustering}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}
  \begin{figure}[h!]
  \caption{\csentence{Sample figure title.}
      A short description of the figure content
      should go here.}
      \end{figure}

\begin{figure}[h!]
  \caption{\csentence{Sample figure title.}
      Figure legend text.}
      \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}
\label{fivewaycomparison}
\begin{table}[h!]
\caption{The resources consumption of the five compared methods as well as the accuracy measured as \ARI{} on chromosome 22.}
      \begin{tabular}{|c|ccc|ccc|c|c|}
        \hline
           Tool &  \multicolumn{3}{ c |}{Pre-processing} & \multicolumn{3}{ c |}{Clustering} & Accuracy \\
& threads & memory & time  &threads & memory & time  & \\
  \hline
\variantSpark{}	& 8	& 32	& 2min 58sec	& 8	& 32	& 1min 20sec	& 0.84	\\ 
ADAM		& 8	& 32	& 12min 48sec	& 8	& 32	& 1min 52sec	& x	\\
Hadoop		& 8	& 32	& 14min 22sec	& 8	& 32	& 14min 23sec	& x	\\
R			& 1	& 32	& 67min		& 8	& 32	& 7min 25sec	& x	\\
Python		& 1	& 32	& 92min		& 8	& 32	& 11min 29sec	& 0.84	\\
  \hline
      \end{tabular}
\end{table}

\label{scalingcomparison}
\begin{table}[h!]
\caption{The resources consumption on different subsets of the entire autosome (chromosomes 1-22). Memory specified is the memory allocated to each executor.}
      \begin{tabular}{|c|ccc|ccc|c|c|}
        \hline
           Portion &  \multicolumn{3}{ c |}{Pre-processing} & \multicolumn{3}{ c |}{Clustering}  \\
& executors & memory & time  & executors & memory & time \\
  \hline
20\%		& 64	& 2	& 11min 53sec	& 64	& 6	& 1hr 10min	\\
40\%		& 64	& 2	& 19min 9sec	& 64	& 12	& 2hr 19min	\\
60\%		& 64	& 2	& 26min 34sec	& 64	& 17	& 3hr 33min	\\
100\%	& 64	& 2	& 40min 48sec	& 40	& 24	& 14hr 44min	\\
  \hline
      \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}
  \subsection*{Additional file 1 --- Sample additional file title}
    Additional file descriptions text (including details of how to
    view the file, if it is in a non-standard format or the file extension).  This might
    refer to a multi-page table or a figure.

  \subsection*{Additional file 2 --- Sample additional file title}
    Additional file descriptions text.


\end{backmatter}
\end{document}
