\documentclass{bioinfo}
\copyrightyear{2005}
\pubyear{2005}

\usepackage[hyphens]{url}

\begin{document}
\firstpage{1}

	
\newcommand{\OnePhaseone}{phase1\_chr1}
\newcommand{\NinteenPhaseone}{phase1\_chr19}
\newcommand{\SevenPhaseone}{phase1\_chr1-7}
\newcommand{\FullPhaseone}{phase1\_chr1-22}
\newcommand{\OnePhasethree}{phase3\_chr1}
\newcommand{\ThreePhasethree}{phase3\_chr1-3}
\newcommand{\FullPhasethree}{phase3\_chr1-22}
\newcommand{\ARI}{Adjusted Rand Index}

\title[MapReduce Clustering]{Scalable Clustering of Genotype Information using MapReduce}
\author[O'Brien \textit{et~al}]{Aidan R O'Brien\,$^{1}$, Neil Saunders\,$^1$,Fabian A Buske\,$^{2}$ and Denis C. Bauer\,$^1$\footnote{to whom correspondence should be addressed}}
\address{$^{1}$CSIRO, Digital Productivity Flagship, 11 Julius Av, 2113, Sydney, Australia\\
$^{2}$Cancer Epigenetics Program, Cancer Research Division, Kinghorn Cancer Centre, Garvan Institute of Medical Research, 384 Victoria St, 2010, Sydney, Australia}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}

\section{Motivation:}
Processing genomic information from whole genome sequence studies pose computational challenges due to the unprecedented data volume generated, which render transitional approaches insufficient. However, by utilising advancements in modern hardware accelerators and data processing we can provide the means for scalable solutions. We therefore aim to provide the interface between standard genomic data formats and advanced and scalable analysis libraries like Mahout. 
\section{Results:}
We achieve an 2-fold speedup by using the scalable k-means MapReduce implementation over the equivalent analysis performed in R, by comparable accuracy. However, the real benefit lies in scaling beyond R's capability to a population-size analysis. We successfully clustered more than 2,500 individuals each having more than 19 Million variants. 

\section{Availability:}
Using modern compute paradigms is essential to scale to modern genomic research in an efficient sustainable way. 

\section{Contact:} \href{Denis.Bauer@CSIRO.au}{Denis.Bauer@CSIRO.au}
\end{abstract}

\section{Introduction}

Grouping individuals based on the genomic profile is a commonly performed tasks to identify population structure~\cite{Gao2007} or elucidate different haplotype involvement in diseases susceptibility~\cite{Laitman2013}.  Traditionally both the number of individuals and included genotypes, typically from SNP arrays, were relatively small and libraries in Bioconductor sufficient. However, recent technological advances in whole genome sequencing have made population-scale sequencing feasible. It is hence economical to generate studies with sample sizes currently reserved for larger consortia such as the 1000 genomes project~\cite{1KG2012} or the cancer genome atlas (TCGA)~\cite{TCGA2013}. At the same time, whole genome sequencing enables the inclusion of rare or even somatic mutations in the analysis, increasing the feature space by orders of magnitude. This drastic increase in both sample numbers and features per sample requires a massively parallel approach to data processing. 

As a result of these big data challenges, MapReduce approaches are increasingly being used in bioinformatics (for reviews see~\cite{Zou2013, Qiu2010,Taylor2010}). This is especially the case for sequence analysis tasks, such as read mapping~\cite{Schatz2009}, duplicate removal~\cite{Jourdren2012}, and variant calling~\cite{Langmead2009, McKenna2010} as well as Genome Wide Analysis Study based tasks~\cite{Huang2013, Guo2014}.

%Mention Goolge's population clustering in the intro

At the same time, theoretical computer scientists adapt MapReduce paradigms to cope with the iterative nature of machine learning tasks~\cite{Chu2009}. Specifically, the Mahout project \url{https://mahout.apache.org/} has been developed extensively~\cite{Ranger2007, Owen2011} and was successfully applied in the clinical informatics space~\cite{Dong2013}.

In this paper we link the two areas of parallel machine learning and bioinformatics by providing an interface between Mahout and the standard variant data format, variant call format (VCF)~\cite{1KG2012}, which opens up the application of Mahout's different machine learning algorithms to be applied to genotype-based tasks. 

To demonstrate the capability we cluster variant datasets from the 1000 genomes project to determine population structure using the k-means clustering algorithm available in Mahout. In the first section we benchmark the performance and accuracy of Mahout's implementations against a standard R based implementation on a reduced version of the data and in the last two sections we investigate how Mahout scales to either more samples or more features per samples.   

%SeqWare~{OConnor2010} Libraries \cite{Doering2008,Schumacher2014,Nordberg2013}


\section*{Results and discussion}

% COMPARISON TO R
\subsection*{Clustering in Mahout requires less memory than in R}
In this section, we compare the runtime and resource requirements between the k-means clustering implementation in R and Mahout for the small \NinteenPhaseone{} dataset (see Methods). 
We measured the runtime separately for pre-processing stage as well as the clustering stage. 

%With Hadoop, pre-processing the data takes approximately 6 minutes, with an allocation of up to 3GB memory per 'container'. The resulting data is stored as a Hadoop 'SequenceFile' of about 835MB in size.
With Hadoop, pre-processing the data takes approximately 6 minutes, with an allocation of up to 3GB memory. 
The resulting data is stored as a Hadoop 'SequenceFile' of about 835MB in size.
Conversely,  pre-processing the data to R's in-memory matrix takes 56 minutes and required 17.7GB memory. 
K-means clustering the data, however, is faster with R, using 7s, whereas Mahout takes just under 9 minutes (see Table~\ref{Tab:02}). 
This is due to the additional overhead involved in spawning the map and reduce processes, and distributing the data between nodes.
R, on the other hand, already has the entire matrix in-memory, on a single machine and since no disk-access or networking is required the clustering can be performed faster. 
Due to the relatively small number of variants, cluster accuracy is relatively poor for both methods, with the best \ARI{} score for clustering individuals in one of 4 populations (see Methods) being 0.705 in Mahout and 0.614 in R (see Table~\ref{Tab:02}).

%Although the overall Mahout pipeline is about 40 minutes faster than R's, the actual k-means clustering takes two orders of magnitude longer in Mahout. 
%This is due to the additional overhead involved in spawning the map and reduce processes, and distributing the data between nodes.
%R, on the other hand, already has the entire matrix in-memory, on a single machine. 
%Since no disk-access or networking is required the clustering can be performed faster. 

While the overall processing time is already 40 minutes faster in Mahout than R, the real bottleneck for larger dataset will be the amount of memory available on a single machine. 
As an example, the entire set of 1000 Genome Project phase3 variants could require approximately 1.6TB to be completely loaded into memory as a double-precision matrix (see Table~\ref{Tab:01}). This is based on the equation:
$\frac{n \times m \times 8\text{bytes}}{1\text{GB}}$, where $n \times m$ are the matrix dimensions, 8bytes is the memory a double occupies (i.e. each matrix element), and  1GB is $10^9$bytes.

Therefore, the data processed in the subsequent sections are too large to be processed with standard R, we hence we focus on evaluating different settings of the Mahout implementation only.  



% (1) SCALING UP FEATURES IN MAHOUT
\subsection*{Hadoop scales to genome-wide clustering}
%In this section we demonstrate Mahout's ability to pre-process and cluster genome-wide variants and its effect on clustering quality.
%We focus on the larger \FullPhasethree{} dataset in this section which has 79 million variants.
%However, we begin with a subset of this dataset, \OnePhasethree{}. Although our algorithm completes successfully, it achieves a relatively low adjusted Rand index of 0.622.
%Next we increase the input to include the first three chromosomes (\ThreePhasethree{}), which has just under 20 million variants. This completes successfully and results in an adjusted Rand index of 0.825.
%Finally we include all 22 autosomes (\FullPhasethree{}) as input to our algorithm. Although the adjusted rand index isn't much higher than the previous example, at 0.84, the pre-processing and k-means complete successfully.
%-Conclusionary statement- 

In this section we demonstrate Mahout's ability to cluster genome-wide variants. 
To demonstrate accuracy-gain and resource-consumption we evaluate whole-genome data (\FullPhasethree{}), which contains 79 million variants, as well as two subsets with 19 (\ThreePhasethree{}) and 6 million (\OnePhasethree{}) variants, respectively.  
As expected, providing variants from only chromosome one (\OnePhasethree{}) achieves a relatively low \ARI{} of 0.622 (see Table~\ref{Tab:02})
Note, phase3 data has one more population group, which explains the decrease in accuracy compared to the previous section.  

Increasing the input to include the first three chromosomes (\ThreePhasethree{}) improves the accuracy with an \ARI{} of 0.825 (Table~\ref{Tab:02}).
We are reaching a saturation point for k-means clustering here, as including all 22 autosomes (\FullPhasethree{}) improves the \ARI{} only marginally to 0.84 (Table~\ref{Tab:02}).
%DB is that really GB or should that be 6.1GB?
However, this demonstrates that performing clustering using whole-genome variants on research infrastructure is feasible using Mahout as the resource consumption lies within realistic boundaries. The time consumption increases to 25 hours and memory to 6 GB (Table~\ref{Tab:02}). 


% (2) SCALING UP SAMPLES IN MAHOUT
\subsection*{Hadoop scales effortlessly to more than twice the number of samples}
%In this section we investigate Hadoop's ability to scale with more samples. 
%Mahout's k-means algorithm performs by splitting clustering jobs up into separate, smaller tasks, where each task calculates the distance between a subset of samples and the k-means centroids.
%Hadoop creates a 'container' for each task, and processes them iteratively until every task is complete.
%In the previous section we increased the sample size. 
%Larger samples have an increased dimensionality, and hence require more memory per container, and also more time to complete.
%In this case, however, we increase the number of samples. 
%With more samples, the k-means algorithm will divide the job up into more tasks, however the memory requirement of each task, and hence, container, should remain the same because the dimensionality is the same.
%Instead the job will take more time because there are more tasks for Mahout to complete.
%In the previous section we increased the sample size. 
%Larger samples have an increased dimensionality, and hence require more memory per container, and also more time to complete.
%In this case, however, we increase the number of samples. 
%With more samples, the k-means algorithm will divide the job up into more tasks, however the memory requirement of each task, and hence, container, should remain the same because the dimensionality is the same.
%Instead the job will take more time because there are more tasks for Mahout to complete.

Hadoop's main advantage over other parallelisation techniques is its ability to scale to more samples without altering the code. 
Mahout's k-means algorithm is hence structured such that it splits the clustering problem up into separate, smaller tasks, where each task calculates the distance between a subset of samples and the k-means centroids. 
The automatic scaling is then achieved by Hadoop creating 'containers' for each task, and processes them iteratively until every task is complete.
We discuss containers in greater detail in the next section, however, a container is essentially a pre-defined allocation of memory and CPU cores.
By increasing the number of samples, the k-means algorithm will simply divide the job up into more tasks, which enables the memory requirement of each task to stay the same. 
This is because the dimensionality is constant, compared to the previous section where we increased the number of variants and observe a subsequent increase in memory. 

To demonstrate this, we compare k-means clustering of the 2504 individuals from the phase3 dataset compared to the 1092 individuals from the phase1 dataset.
We specifically use the \SevenPhaseone{} and \ThreePhasethree{} datasets because of their similar feature size.
%The algorithm completed successfully with the same sized containers for both datasets, even with more than twice the number of samples in the larger dataset.
%However, the \ThreePhasethree{} dataset required about twenty minutes more time than the smaller \SevenPhaseone{} dataset. 
The algorithm completed successfully with the same sized containers for both datasets, however, the \ThreePhasethree{} dataset required about twenty minutes more time than the smaller \SevenPhaseone{} dataset (Table~\ref{Tab:02}). 
This demonstrates Hadoop's strength of scaling with sample size. 


%Rather than the larger comparison requiring us to allocate more resources allocated to containers, it simply required more time. Therefore, increasing the sample size of a job should not require reconfiguration of Hadoop or a larger cluster.


%% OLD
%The container size required for pre-processing the data is identical for both datasets, however the \ThreePhasethree{} dataset required about twenty minutes more time than the smaller \SevenPhaseone{} dataset.
%The clustering accuracy is 0.813 for the phase1 dataset and 0.825 for the phase3 dataset.
%%In this case, due to the number of Feature Vectors increasing, rather than the feature size, we expect the memory requirements for k-means clustering should remain similar, as the cluster centres loaded into the containers' memory are approximately the same size.
%%We expect the memory requirement for the k-means clustering to stay the same as the number of cluster centres loaded into the containers' memory remain constant despite the increase of feature vectors oppose to feature size.
%We expect the memory requirement for the k-means clustering algorithm to be the similar for both datasets, because the larger size of \SevenPhaseone{} is a result of more feature vectors (more individuals), rather than larger feature vectors (more variants).
%Mahout's iterative k-means algorithm loads a set number of these vectors into memory, based on the number of k-means clusters. And therefore, if the feature size doesn't increase, this memory requirement also won't increase.
%This proved to be the case, as 4GB allocated per container was the minimum for both datasets.

%%DB: do you agree with this concluding statement?
%%Scaling effortless to massively more samples is the strength of Hadoop, hence compute problems need to be carefully designed to harness this ability.  
%%Hadoop's strength is billed to be its ability to scale to massively more samples, however on a finite-resource cluster we observe a limitation to this scalability, which results in a dramatic increase in runtime. 
%Hadoop's strength is billed to be its ability to scale to massively more samples. We observe this to be true, as increasing the samples by two and a half, which is the case from \SevenPhaseone{} to \ThreePhasethree{}, does not result in greater memory requirements,
%unlike increasing the number of variants per sample.

% (3) CLUSTER AND CONTAINER SIZE
\subsection*{Cluster size and containers}
In the previous section we observed that while the memory remained constant the runtime increased for larger sample sizes. 
On a larger cluster the runtime would also have remained constant. 
%When designing a MapReduce job, it is important that the user specify container size correctly.
However, as research is carried out on resource-finite infrastructure, we discuss in this section how to specify the optimal container size when designing MapReduce jobs to incur only a minimal increase in runtime. 
%Overallocation of resources can result in wasted resources, whereas under allocation can result in failed jobs.

To elaborate, a container is essentially an abstraction of resources, i.e. memory and CPU cores. 
After Hadoop divides a job up into tasks, each task runs in parallel in a container. Also, each container is independent of other containers.
The resources Hadoop allocates to each container is a predefined value, specified by the user in Hadoop's configuration. 
% DB reference or version
According to the Hortonworks documentation (\cite{Horton2013}), the general rule of thumb is that one CPU core is allocated to each container, and the available memory per node should be divided equally between each of those containers. 
For example, on a node with 8 CPU cores and 32GB memory, each container should be allocated 1 CPU core and 4GB memory. Effectively allowing a maximum of eight 4GB containers to run in parallel on that node.

When a MapReduce job is launched, Hadoop spawns a number of map tasks and reduce tasks. 
These tasks are placed in a queue until a node has enough resources available to run a container, and generally, the more containers that can run simultaneously, the faster the job will complete.
For smaller jobs, the maximum number of containers are generally limited by the available CPU cores because each container requires an entire CPU core. The tasks running in each container may not be able to utilise all of the memory allocated to that container.
However, for jobs that deal with large datasets, the memory allocated per container (following Cloudera's guidelines) may not be enough.
We could modify the above example to allocate each container 1 CPU core and 8GB memory, however now only four containers can fit within the node's 32GB memory, resulting in the other 4 CPU cores being disused (because only four containers can run simultaneously). It is therefore optimal to
allocate the appropriate resources to containers, and only increase container memory when necessary, as overallocation of memory will simply limit the number of simultaneous containers each node can run.

To demonstrate the importance of specifying these parameters correctly, we ran identical jobs, using different Hadoop configurations. 
The \OnePhaseone{} dataset only requires 1.5GB memory per container, as we demonstrated previously.
%Increasing the map container size to 4GB results in the job taking an additional 5 minutes (see Table \ref{Tab:03}).
Increasing the map container size to 4GB results in the job taking an additional 5 minutes (see Table~\ref{Tab:03}), because fewer tasks can run in parallel. 
%This larger container size is still too small for the \FullPhasethree{} dataset, which we demonstrated to require 6GB memory per container. 
%Running a job on this dataset with 4GB memory per container results in the job failing.
Attempting to run the larger data set (\FullPhasethree{}) with either 1.5GB or 4GB will result in the job failing, because the feature vectors cannot be loaded completely, this requires at least 6GB memory per container and 21h runtime. 

One of the main selling points of Hadoop, is that it will run on commodity hardware. 
%For this reason we used a relatively small cluster for our benchmarks. Although our cluster uses VMs on Microsoft Azure, it would be relatively simple for a lab to
%create a similar Hadoop setup in-house. 
To mimic the available resource in a standard resource setting we used only a small VM cluster on Microsoft Azure for the benchmarking in this paper so far.
However, to demonstrate the scaling ability of Hadoop and our MapReduce algorithms, we also ran the clustering jobs on a much larger development Hadoop cluster. 
%As presented in Table \ref{Tab:03}, the \OnePhaseone{} job saw an improvement of approximately 70\%.
%The \FullPhasethree{} example, on the other hand, was 78\% faster.
As presented in Table~\ref{Tab:03}, the \OnePhaseone{} job saw an improvement of approximately 70\% when providing 448 cores of 13GB. 
With these resources, the \FullPhasethree{} example, was 78\% faster.

Although both jobs were more than twice as fast on the larger cluster, the speed doesn't scale relative to the size of the cluster. This is due to a limitation in these examples, where the number of reduce tasks in k-means clustering is limited to the number of k-means clusters (k).
A task where k is five, for example, will only be able to spawn five reduce tasks, no matter how many resources are available. Although sometimes inevitable, this bottleneck of maximum simultaneous reduce tasks is an important consideration to take into account when designing MapReduce jobs. 

\begin{figure}[!tpb]%figure1
\centerline{\includegraphics[type=pdf,ext=.pdf,read=.pdf, scale=0.40]{gephi-p3}}
        \label{fig:sign}
        \caption{{\bf Visualisation of k-means clusters derived from the phase 3 dataset.}
        Nodes are coloured by super population: AFR (green), AMR (blue), EAS (pink), EUR (red) and SAS (cyan). 
}
\end{figure}


\subsection*{Result visualisation}
% DB: in the conclusion we discussed that we provide a "toolset to interpret Mahout's clustering output". 
% DB: can we discuss here how we get a visual representation from the clustering (ClusterDumper+Gephi) and what the image tells us..
GraphML files generated using Mahout Cluster Dumper can be visualised using popular freely-available tools such as Gephi~\cite{ICWSM09154}. An example visualisation using the phase 3 dataset, with nodes coloured by super population, is shown in Figure 1.

The clusters generated by k-means clustering and their population structure are readily apparent. Three clusters are homogeneous, containing only EAS (1 cluster) or SAS individuals (2 clusters, the second small and unconnected). Two clusters contain predominantly one super population plus a minor component of another: AMR + AFR or AFR + AMR. The sixth cluster is predominantly a mix of EUR + AMR, with 3 AFR individuals and 1 SAS.



\section*{Conclusion}
In this paper we have applied Mahout's k-means clustering to the task of clustering individuals, based on their genomic variants. 
We provide an interface from VCF format to Hadoop and Mahout, and a toolset to interpret Mahout's clustering output (see Figure 2)
We find that on small datasets the clustering stage is two orders of magnitudes slower in Mahout compared to R (9 minutes vs 7 seconds), due to the overhead of setting up and co-ordinating the communication between the Hadoop nodes. 
However, increasing either the number of samples or the number of features per sample quickly exceeds the capability of R on this machine due to memory limitations.
%DB ok effortlessly might be overstated ?
Hadoop on the other hand scales effortlessly to more than twice the number samples (2,504). It can also process VCF files that contain over 79 Million variants per samples, albeit with higher memory allocation.
The MapReduce functions can scale from a relatively small and accessible cluster with 16 cores, to a much larger cluster with close to 500 CPU cores. 
In conclusion, Mahout offers a scalable out-of-the box solution that is directly applicable to genotype-based clustering for population or family association. 

%This efficient a high performance in-memory map-reduce system has already demonstrated to deliver a  50x speed-up on a whole-genome dataset compared to usual multithreaded approaches, see ADAM~\cite{Massie2013}


\begin{methods}
\section{Methods}
\subsection*{Datasets}
We used variant call format (VCF) files from phases 1 and 3 of the 1000 Genomes Project.
Each VCF file details the genetic variation (SNPs and indels) for one chromosome, between each individual and a reference genome derived from GRCh37. 
The phase1 and phase3 datasets contain 1,092 and 2,504 individuals, respectively.
Metadata available from the 1000 Genomes Consortium specifies additional attributes for each individual, including population and familial data.\\
We used different subsets of the data for different comparisons. For example, to be able to compare Hadoop with R, we used the VCF file containing chromosome 19 variants (\NinteenPhaseone{}).
To demonstrate scaling with Hadoop, we use subsets that contain more variants, i.e. \SevenPhaseone{} and \FullPhaseone{}, which include chromosome 1-7 and the full genome, respectively.
Table~\ref{Tab:01} contains an overview of the datasets.

\subsection*{Hardware and Software}
%DB what about Piotr's cluster I thought you used that one for the full genome data of phase 1 and phase 3 ?
We processed the data using a cluster of five VMs on the Microsoft Azure cloud service. Three of these VMs, the 'worker nodes', each have 14GB memory and and 8 CPU cores. These three VMs run the Hadoop2 NodeManager, which manages the Yet Another Resource Negotiator (YARN) 'containers' (see discussion).
The user is able to specify the resources allocated to each container, where map containers can have a different allocation to reduce containers.
The fourth VM runs the HDFS NameNode, ResourceManager and monitoring services, and the fifth VM, a small, single core machine, runs the Cloudera manager service.
With a total of 42GB memory and 24 CPU cores available between the node managers, we can simultaneously run up to 24 YARN containers with 1.5GB memory per container, or 12 YARN containers with 3GB memory per container.
Storage consists of a 3TB Hadoop Distributed File System (HDFS) partition distributed across the three worker nodes. Each of these nodes runs a DataNode with a 1TB disk.

The larger cluster mainly differs in regards to hardware. It has a total of 448 CPU cores and 1.31TB memory available. 

We used a single VM for the comparison between Mahout and R.

\subsection*{Pre-processing}
\label{Sec:preprocessing}
Mahout requires Hadoop's `SequenceFiles' as input to its machine learning algorithms. SequenceFiles are binary flat files consisting of key/value pairs. In this case, each of the keys is a
label for an individual, where the associated value for each key is a `sparse vector' representing the individual's genomic variants. We use sparse vectors, rather than
dense vectors, to significantly reduce required memory and disk space. Each sparse vector consists of two separate vectors. An item in the first vector is
effectively a key for an item at the same position in the second vector. The key represents a genomic location, where its value represents the variant. Zero values (positions where there is no variant)
are omitted. Therefore, the fewer variants in an individual's genome, the smaller the sparse vector size. In comparison, `dense vectors' are single arrays that
include every position where a variant may fall, whether or not there actually is a variant. Therefore, the size of a dense vector isn't reduced with a large number of non-variants.

The values of the sparse vectors, which, as previously mentioned, represent an individuals genomic variants, are a \texttt{double} java type.
We set the value of each double to the sum of the square root of the Hamming distance between each variant allele and the reference genome. Therefore, a value identical to the reference, 0\textbar{}0, will be 0 (and omitted from the sparse vector),
a heterozygous variant, 0\textbar{}1 or 1\textbar{}0, will be 1 (sqrt1) and a homozygous variant, 1\textbar{}1, will be 2 (sqrt1 + sqrt1). The square root of the each allele prevents variants such as 1\textbar{}1 colliding with the variant 2\textbar{}0.

We wrote two `MapReduce' classes in Java to convert VCF files to the aforementioned sequence files. This is scalable and should work on any number of files,
of any file size; limited only by disk space and time. 
The `map' stage of the first job parses each line of the VCF file to key value pairs. Each of the keys are an identical arbitrary number, where the value for each key is a sparse vector of the variants from each line. The variants in each vector are in the same order as they appear in the VCF file.

Additionally, in this stage, we can define parameters to exclude certain variants that may be deemed low quality, or only appear in a certain number of individuals.
We use identical keys so that each key value pair is sent to the same reducer, and this reducer assigns a unique key to each value. We require one reducer in this step to make each the key for each variant a consecutive number, rather than an arbitrary feature, such as its genomic location. Because keys are represented by integers, this helps to avoid integer overflows by keeping the keys relatively small.

The second job effectively transposes the output from the first job. The mappers create key value pairs where the key is a composite of the individual id and variant id, and the value is the variant.
This composite key, which is essentially a concatenation of two keys, allowed us to implement custom 'collect' and 'sort' classes to speed up the process. These classes divide up the key value pairs by their individual ID and sort them by their location ID, so each sorted collection of key/value pairs can be sent to a separate reducer.
 The reducers then write the variants for each individual to a sparse vector and stores each vector as a value, where the key is the individual ID.


\subsection*{Mahout K-means clustering}
To cluster the samples from the sequence files, we use Mahout 0.9 with Hadoop 2.5.0. 
The clustering steps are the same for each dataset, however, due to the increased feature size we designate more memory to the
YARN containers. For the first step, we specify \texttt{k}, the number of clusters. The function \texttt{RandomSeedGenerator.buildRandom} creates a new sequence file containing \texttt{k}
random centroids from the samples sequence file. The sequence file
of samples and the sequence file of centroids serve as the input to \texttt{KMeansDriver.run}, a static Mahout API function that launches
a k-means clustering job on the Hadoop service. 


\subsection*{R K-means clustering}
We use `readVcf' from the R library, `VariantAnnotation', to iteratively read in the VCF file. For each iteration, we convert the data to a matrix and
convert the values for each variant to a double, as mentioned previously. We then transpose the matrix, so individuals are represented
by rows, and append the matrix to that from the previous iteration. Once the matrix contains all of the variants from the VCF file, we cluster the data
using the `kmeans' algorithm from the `stats' library.



\subsection*{Clustering quality}
We scored the clusters with the \ARI{}~\cite{Hubert1985}. 
This index compares two different cluster sets, here the prediction $X = \{ X_1, X_2, \ldots , X_r \}$ and the annotated data $Y = \{ Y_1, Y_2, \ldots , Y_s \}$, and assigns a value based the overlap between $X$ and $Y$ as captured in a contingency table $\left[n_{ij}\right]$ where each entry $n_{ij}$ denotes the number of objects in common between $X_i$ and $Y_j : n_{ij}=|X_i \cap Y_j|$. 
The \ARI{} is then 
{\tiny
\begin{eqnarray*}
ARI=\frac{\sum_{ij}{{n_{ij}\choose 2}} - \left[ \sum_{i}{{a_i\choose2}} \sum_{j}{{b_i\choose2}} \right] / {n\choose2}}{\frac{1}{2} \left[ \sum_{i}{{a_{i}\choose 2} + \sum_{j}{{b_{j}\choose 2}}} \right] - \left[ \sum_{i}{{a_{i}\choose 2} + \sum_{j}{{b_{j}\choose 2}}} \right] / {n\choose2}} 
\end{eqnarray*}
}
where $a$ and $b$ are the sums over the rows and columns of the contingency table.

The value is zero for independent clusterings and one for identical clusterings. 
We compared the Mahout k-means clusters and the R k-means clusters to the known clusters based on meta data from the 1000 genome project.
For the known clusters, individuals from the both phase1 and phase3 datasets were partitioned by their super population code, ASN, EUR, AFR, and AMR, where phase3 included EAS and SAS instead of ASN.



\subsection*{Performance}
We recorded the job time from the 'Elapsed' time as reported by the Hadoop JobHistory server. This is measured from the job's start time to finish time.
Because k-means clustering performs multiple iterations, and therefore spawns multiple MapReduce jobs, the time we reported is the sum of the elapsed time of each job.
For the R job, where we could not easily separate the pre-processing and clustering components, we printed the time between
stages using the function, \texttt{proc.time()}.


\subsection*{Graph visualisation}
GraphML files generated using Mahout Cluster Dumper were visualised using Gephi. Layouts were applied in the following order: Random, ForceAtlas 2, Yifan Hu Proportional. For ForceAtlas 2 behaviour alternatives ``dissuade hubs'', ``linlog mode'' and ``prevent overlap'' were set to true, ``approximate repulsion'' was set to false.


\end{methods}

\begin{table}[!t]
\processtable{Datasets used in this study.\label{Tab:01}}
{\begin{tabular}{lrrrrr}\toprule
name& individuals & variants & in-memory  & population\\
& & &size (GB) &groups& \\\midrule
        \NinteenPhaseone{} & 1,092 & 816,115 & 7.13  & 4\\
        \OnePhaseone{} & 1,092 & 3,007,196 & 26.27  & 4\\
        \SevenPhaseone{} & 1,092 & 18,984,880 & 165.85 & 4\\
        \FullPhaseone{} & 1,092 & 38,219,238 & 333.88 & 4\\
	\OnePhasethree{} & 2,504 & 6,468,094 & 131.17 & 5\\
	\ThreePhasethree{} & 2,504 & 19,381,970 & 393.06 & 5\\
	\FullPhasethree{} & 2,504 & 81,271,745 & 1602.00 & 5\\\botrule
\end{tabular}}{Datasets used in this study; based on portions of the phase1 and phase3 data from the 1000 genomes project.
In-memory size is the approximate memory required to store the data as a double-precision matrix of size n,m using the formula: n*m*8bytes/10\textsuperscript{9}bytes.
}
\end{table}

\begin{table*}[!t]
\processtable{Resources allocated and time taken.\label{Tab:02}}
{\begin{tabular}{ll|cc|cc|rr|c}\toprule
Tool & Dataset & \multicolumn{2}{ c |}{Preprocesing (MB)} & \multicolumn{2}{ c |}{Clustering (MB)} & \multicolumn{2}{ c |}{Time} & Accuracy\\
& & map & reduce  & map & reduce  & pre-processing & clustering & \\\midrule
        R &\NinteenPhaseone{} & - & - & - & - & 55m 41s & 7s &0.614\\
        Mahout & \NinteenPhaseone{} & 1536 & 3072 & 1536 & 3072 & 6m 10s & 8m 32s & 0.705\\
        & \OnePhaseone{} & 1536 & 3072 & 1536 & 3072 & 9m 52s & 12m 49s & 0.798\\
        & \SevenPhaseone{} & 1536 & 3072 & 4096 & 4096  & 43m 56s & 3h 28m & 0.813\\
       % & \FullPhaseone{} & 1536 & 3072 & 6144 & 6144  & 85m 29s & 7h 43m & 0.723\\
        & \OnePhasethree{} & 1536 & 3072 & 1536 & 3072 & 23m 27s & 21m 14s & 0.622\\
        & \ThreePhasethree{} & 1536 & 3072 & 4096 & 4096 & 63m 49s & 4h 21m & 0.825\\
        & \FullPhasethree{} & 1536 & 3072 & 6144 & 6144 & 5h 36m & 21h 5m &0.840\\\botrule
\end{tabular}}{The resources allocated to each MapReduce yarn-container and time taken for the job(s) to complete as well as the accuracy measured as \ARI{}.}
\end{table*}


\begin{table*}[!t]
\processtable{Resources allocated and time taken.\label{Tab:03}}
{\begin{tabular}{c|cc|cc|c}\toprule
Dataset & \multicolumn{2}{ c |}{Available Resources} & \multicolumn{2}{ c |}{Container Size (MB)} & Time\\
& CPU cores & Memory (GB)  & map & reduce & \\\midrule
        \OnePhaseone{} & 24 & 36 & 1536 & 3072 & 12m 49s\\
        \OnePhaseone{} & 24 & 36 & 4096 & 4096  & 17m 47s\\
        \OnePhaseone{} & 448 & 1310 & 1536 & 4096  & 5m 22s\\
        \FullPhasethree{} & 24 & 36 & 6144 & 6144 & 21h 4m\\
        \FullPhasethree{} & 448 & 1310 & 6144 & 6144 & 4h 42m\\\botrule
\end{tabular}}{The resources allocated to each MapReduce yarn-container and time taken for the job(s) to complete.}
\end{table*}


\begin{figure}[!tpb]%figure2
\centerline{\includegraphics[type=pdf,ext=.pdf,read=.pdf, scale=0.40]{signature}}
        \label{fig:sign}
        \caption{{\bf Illustration of Mahout-based clustering of genotypes.}
      The image shows how the here introduced interface converts the VCF file to a mahout usable vector-based data type. Based on k-means it illustrates how the Map-step procedure groups of vectors by their nearest cluster center and the Reduce-step averages the vectors in each group to find the new value of the updated cluster center. The mahout-produced output is then converted into a visualisation.}

\end{figure}





\section*{Acknowledgement}
A.R.O was funded by the NSW Cancer Institute Big Data Big Impact schema, F.A.B by the National Health and Medical Research Council [1051757] and D.C.B by Commonwealth Scientific and Industrial Research Organisation's Transformational Capability Platform, Science and Industry Endowment Fund and Information Management and Technology Services.
The authors would like to thank Piotr Szul and Gareth Williams for their help with setting up Hadoop on the HPC system.  

\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
\bibliography{genotypeClustering}  
\end{document}
