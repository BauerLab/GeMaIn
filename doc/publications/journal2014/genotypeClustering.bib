% This file was created with JabRef 2.10.
% Encoding: MacRoman


@Article{1KG2012,
  Title                    = {An integrated map of genetic variation from 1,092 human genomes.},
  Author                   = {{1000 Genomes Project Consortium}},
  Journal                  = {Nature},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {7422},
  Pages                    = {56--65},
  Volume                   = {491},

  Bdsk-url-1               = {http://dx.doi.org/10.1038/nature11632},
  Doi                      = {10.1038/nature11632},
  Keywords                 = {Alleles; Binding Sites, genetics; Conserved Sequence, genetics; Continental Population Groups, genetics; Evolution, Molecular; Genetic Variation, genetics; Genetics, Medical; Genetics, Population; Genome, Human, genetics; Genome-Wide Association Study; Genomics; Haplotypes, genetics; Humans; Nucleotide Motifs; Polymorphism, Single Nucleotide, genetics; Sequence Deletion, genetics; Transcription Factors, metabolism},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {nature11632},
  Pmid                     = {23128226},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/nature11632}
}

@InProceedings{ICWSM09154,
  Title                    = {Gephi: An Open Source Software for Exploring and Manipulating Networks},
  Author                   = {Mathieu Bastian and Sebastien Heymann and Mathieu Jacomy},
  Booktitle                = {Proceedings of the International AAAI Conference on Weblogs and Social Media},
  Year                     = {2009},

  Abstract                 = {Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results. We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.},
  Bdsk-url-1               = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154},
  Conference               = {International AAAI Conference on Weblogs and Social Media},
  Date-modified            = {2015-03-11 05:47:43 +0000},
  Keywords                 = {network;network science;visualization;graph exploration;open source;free software;dynamic network;interactive interface;graph;force vector;java;OpenGL;3-D visualization;user-centric;graph layout;complex graph rendering;network analysis;webatlas},
  Url                      = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154}
}

@Electronic{Borthakur2007,
  Title                    = {The hadoop distributed file system: Architecture and design},
  Author                   = {Dhruba Borthakur},
  Month                    = {11},
  Organization             = {Hadoop Project Website},
  Year                     = {2007},

  Owner                    = {bau04c},
  Timestamp                = {2015.08.14}
}

@Article{TCGA2013,
  Title                    = {The Cancer Genome Atlas Pan-Cancer analysis project.},
  Author                   = {{Cancer Genome Atlas Research Network}},
  Journal                  = {Nat Genet},
  Year                     = {2013},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1113--1120},
  Volume                   = {45},

  Abstract                 = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed large numbers of human tumors to discover molecular aberrations at the DNA, RNA, protein and epigenetic levels. The resulting rich data provide a major opportunity to develop an integrated picture of commonalities, differences and emergent themes across tumor lineages. The Pan-Cancer initiative compares the first 12 tumor types profiled by TCGA. Analysis of the molecular aberrations and their functional roles across tumor types will teach us how to extend therapies effective in one cancer type to others with a similar genomic profile.},
  Bdsk-url-1               = {http://dx.doi.org/10.1038/ng.2764},
  Doi                      = {10.1038/ng.2764},
  Keywords                 = {Gene Expression Profiling; Genome; Humans; Neoplasms, genetics/pathology},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {ng.2764},
  Pmid                     = {24071849},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/ng.2764}
}

@InProceedings{Chu2009,
  Title                    = {Map-Reduce for Machine Learning on Multicore},
  Author                   = {Chu, Cheng T. and Kim, Sang K. and Lin, Yi A. and Yu, Yuanyuan and Bradski, Gary R. and Ng, Andrew Y. and Olukotun, Kunle},
  Booktitle                = {NIPS},
  Year                     = {2006},
  Editor                   = {Schlkopf, Bernhard and Platt, John C. and Hoffman, Thomas},
  Pages                    = {281--288},
  Publisher                = {MIT Press},

  Added-at                 = {2009-08-17T16:24:20.000+0200},
  Bdsk-url-1               = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06},
  Biburl                   = {http://www.bibsonomy.org/bibtex/20de668cfdfc349fe197f30c7d840ba0f/sb3000},
  Citeulike-article-id     = {2308503},
  Citeulike-linkout-0      = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06},
  Citeulike-linkout-1      = {http://www.cs.stanford.edu/people/ang//papers/nips06-mapreducemulticore.pdf},
  Description              = {CiteULike: Map-Reduce for Machine Learning on Multicore},
  Interhash                = {96ced10b0cf1c9ce041cf2b43574656c},
  Intrahash                = {0de668cfdfc349fe197f30c7d840ba0f},
  Keywords                 = {cloud mapreduce ml},
  Posted-at                = {2008-03-07 03:16:12},
  Priority                 = {0},
  Timestamp                = {2009-08-17T16:24:20.000+0200},
  Url                      = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06}
}

@Article{Doering2008,
  Title                    = {SeqAn an efficient, generic C++ library for sequence analysis.},
  Author                   = {D{\"o}ring, Andreas and Weese, David and Rausch, Tobias and Reinert, Knut},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2008},
  Pages                    = {11},
  Volume                   = {9},

  Abstract                 = {The use of novel algorithmic techniques is pivotal to many important problems in life science. For example the sequencing of the human genome 1 would not have been possible without advanced assembly algorithms. However, owing to the high speed of technological progress and the urgent need for bioinformatics tools, there is a widening gap between state-of-the-art algorithmic techniques and the actual algorithmic components of tools that are in widespread use.To remedy this trend we propose the use of SeqAn, a library of efficient data types and algorithms for sequence analysis in computational biology. SeqAn comprises implementations of existing, practical state-of-the-art algorithmic components to provide a sound basis for algorithm testing and development. In this paper we describe the design and content of SeqAn and demonstrate its use by giving two examples. In the first example we show an application of SeqAn as an experimental platform by comparing different exact string matching algorithms. The second example is a simple version of the well-known MUMmer tool rewritten in SeqAn. Results indicate that our implementation is very efficient and versatile to use.We anticipate that SeqAn greatly simplifies the rapid development of new bioinformatics tools by providing a collection of readily usable, well-designed algorithmic components which are fundamental for the field of sequence analysis. This leverages not only the implementation of new algorithms, but also enables a sound analysis and comparison of existing algorithms.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-9-11},
  Doi                      = {10.1186/1471-2105-9-11},
  Institution              = {Algorithmische Bioinformatik, Institut fÂ¸r Informatik, Takustr, 9, 14195 Berlin, Germany. doering@inf.fu-berlin.de},
  Keywords                 = {Algorithms; Database Management Systems; Databases, Genetic; Programming Languages; Sequence Alignment, methods; Sequence Analysis, methods; Software; User-Computer Interface},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-9-11},
  Pmid                     = {18184432},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-9-11}
}

@Article{Dong2013,
  Title                    = {Leverage hadoop framework for large scale clinical informatics applications.},
  Author                   = {Dong, Xiao and Bahroos, Neil and Sadhu, Eugene and Jackson, Tommie and Chukhman, Morris and Johnson, Robert and Boyd, Andrew and Hynes, Denise},
  Journal                  = {AMIA Summits Transl Sci Proc},
  Year                     = {2013},
  Pages                    = {53},
  Volume                   = {2013},

  Institution              = {University of Illinois at Chicago, Chicago, IL.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pmid                     = {24303235},
  Timestamp                = {2014.05.01}
}

@Article{Gao2007,
  Title                    = {Human population structure detection via multilocus genotype clustering.},
  Author                   = {Gao, Xiaoyi and Starmer, Joshua},
  Journal                  = {BMC Genet},
  Year                     = {2007},
  Pages                    = {34},
  Volume                   = {8},

  Abstract                 = {We describe a hierarchical clustering algorithm for using Single Nucleotide Polymorphism (SNP) genetic data to assign individuals to populations. The method does not assume Hardy-Weinberg equilibrium and linkage equilibrium among loci in sample population individuals.We show that the algorithm can assign sample individuals highly accurately to their corresponding ethnic groups in our tests using HapMap SNP data and it is also robust to admixed populations when tested with Perlegen SNP data. Moreover, it can detect fine-scale population structure as subtle as that between Chinese and Japanese by using genome-wide high-diversity SNP loci.The algorithm provides an alternative approach to the popular STRUCTURE program, especially for fine-scale population structure detection in genome-wide association studies. This is the first successful separation of Chinese and Japanese samples using random SNP loci with high statistical support.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2156-8-34},
  Doi                      = {10.1186/1471-2156-8-34},
  Institution              = {Miami Institute for Human Genomics, University of Miami Miller School of Medicine, Miami, FL 33136, USA. xgao@med.miami.edu},
  Keywords                 = {Algorithms; Asian Continental Ancestry Group, classification/genetics; Cluster Analysis; Computer Simulation; Gene Frequency; Genetic Markers; Genetics, Population; Genotype; Humans; Models, Genetic; Polymorphism, Single Nucleotide},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2156-8-34},
  Pmid                     = {17592628},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2156-8-34}
}

@Article{Guo2014,
  Title                    = {Cloud computing for detecting high-order genome-wide epistatic interaction via dynamic clustering.},
  Author                   = {Guo, Xuan and Meng, Yu and Yu, Ning and Pan, Yi},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {102},
  Volume                   = {15},

  Abstract                 = {Taking the advan tage of high-throughput single nucleotide polymorphism (SNP) genotyping technology, large genome-wide association studies (GWASs) have been considered to hold promise for unravelling complex relationships between genotype and phenotype. At present, traditional single-locus-based methods are insufficient to detect interactions consisting of multiple-locus, which are broadly existing in complex traits. In addition, statistic tests for high order epistatic interactions with more than 2 SNPs propose computational and analytical challenges because the computation increases exponentially as the cardinality of SNPs combinations gets larger.In this paper, we provide a simple, fast and powerful method using dynamic clustering and cloud computing to detect genome-wide multi-locus epistatic interactions. We have constructed systematic experiments to compare powers performance against some recently proposed algorithms, including TEAM, SNPRuler, EDCF and BOOST. Furthermore, we have applied our method on two real GWAS datasets, Age-related macular degeneration (AMD) and Rheumatoid arthritis (RA) datasets, where we find some novel potential disease-related genetic factors which are not shown up in detections of 2-loci epistatic interactions.Experimental results on simulated data demonstrate that our method is more powerful than some recently proposed methods on both two- and three-locus disease models. Our method has discovered many novel high-order associations that are significantly enriched in cases from two real GWAS datasets. Moreover, the running time of the cloud implementation for our method on AMD dataset and RA dataset are roughly 2 hours and 50 hours on a cluster with forty small virtual machines for detecting two-locus interactions, respectively. Therefore, we believe that our method is suitable and effective for the full-scale analysis of multiple-locus epistatic interactions in GWAS.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-15-102},
  Doi                      = {10.1186/1471-2105-15-102},
  Institution              = {Department of Computer Science, Georgia State University, 34 Peachtree Street, Atlanta, USA. yipan@gsu.edu.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-15-102},
  Pmid                     = {24717145},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-15-102}
}

@Misc{Horton2013,
  Title                    = {Hortonworks Data Platform},

  Author                   = {Hortonworks},
  Note                     = {[Online; accessed 6 March 2015]},
  Year                     = {2013},

  Bdsk-url-1               = {http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/bk_installing_manually_book-20131022.pdf},
  Date-added               = {2015-03-06 07:19:05 +0000},
  Date-modified            = {2015-03-11 05:40:03 +0000},
  LastChecked              = {6 March 2015},
  Url                      = {http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/bk_installing_manually_book-20131022.pdf},
  Urldate                  = {22 October 2013}
}

@Article{Huang2013,
  Title                    = {BlueSNP: R package for highly scalable genome-wide association studies using Hadoop clusters.},
  Author                   = {Huang, Hailiang and Tata, Sandeep and Prill, Robert J.},
  Journal                  = {Bioinformatics},
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {135--136},
  Volume                   = {29},

  Abstract                 = {Computational workloads for genome-wide association studies (GWAS) are growing in scale and complexity outpacing the capabilities of single-threaded software designed for personal computers. The BlueSNP R package implements GWAS statistical tests in the R programming language and executes the calculations across computer clusters configured with Apache Hadoop, a de facto standard framework for distributed data processing using the MapReduce formalism. BlueSNP makes computationally intensive analyses, such as estimating empirical p-values via data permutation, and searching for expression quantitative trait loci over thousands of genes, feasible for large genotype-phenotype datasets. Availability and implementation: http://github.com/ibm-bioinformatics/bluesnp},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/bts647},
  Doi                      = {10.1093/bioinformatics/bts647},
  Institution              = {Healthcare Informatics, IBM Almaden Research Center, San Jose, CA 95120, USA.},
  Keywords                 = {Genome-Wide Association Study; Humans; Phenotype; Quantitative Trait Loci; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {bts647},
  Pmid                     = {23202745},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/bts647}
}

@Article{Hubert1985,
  Title                    = {Comparing partitions},
  Author                   = {Hubert, Lawrence and Arabie, Phipps},
  Journal                  = {Journal of Classification},
  Year                     = {1985},
  Number                   = {1},
  Pages                    = {193-218},
  Volume                   = {2},

  Bdsk-url-1               = {http://dx.doi.org/10.1007/BF01908075},
  Date-added               = {2014-05-17 04:09:59 +0000},
  Date-modified            = {2014-05-17 04:10:33 +0000},
  Doi                      = {10.1007/BF01908075},
  ISSN                     = {0176-4268},
  Keywords                 = {Measures of agreement; Measures of association; Consensus indices},
  Language                 = {English},
  Publisher                = {Springer-Verlag},
  Url                      = {http://dx.doi.org/10.1007/BF01908075}
}

@Article{Hunter2014,
  Title                    = {The genetics of human migrations: Our ancestors migration out of Africa has left traces in our genomes that explain how they adapted to new environments.},
  Author                   = {Hunter, Philip},
  Journal                  = {EMBO Rep},
  Year                     = {2014},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1019--1022},
  Volume                   = {15},

  __markedentry            = {[bau04c:6]},
  Doi                      = {10.15252/embr.201439469},
  Institution              = {Freelance journalist in London, UK.},
  Keywords                 = {Adaptation, Biological, genetics; Africa; Biological Evolution; Continental Population Groups, genetics; Environment; Genome, Human; Human Migration; Humans},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {embr.201439469},
  Pmid                     = {25216943},
  Timestamp                = {2015.09.10},
  Url                      = {http://dx.doi.org/10.15252/embr.201439469}
}

@Article{Jourdren2012,
  Title                    = {Eoulsan: a cloud computing-based framework facilitating high throughput sequencing analyses.},
  Author                   = {Jourdren, Laurent and Bernard, Maria and Dillies, Marie-Agn{\"E}s and {Le Crom}, St{\`E}phane},
  Journal                  = {Bioinformatics},
  Year                     = {2012},

  Month                    = {Jun},
  Number                   = {11},
  Pages                    = {1542--1543},
  Volume                   = {28},

  Abstract                 = {We developed a modular and scalable framework called Eoulsan, based on the Hadoop implementation of the MapReduce algorithm dedicated to high-throughput sequencing data analysis. Eoulsan allows users to easily set up a cloud computing cluster and automate the analysis of several samples at once using various software solutions available. Our tests with Amazon Web Services demonstrated that the computation cost is linear with the number of instances booked as is the running time with the increasing amounts of data. Availability and implementation: Eoulsan is implemented in Java, supported on Linux systems and distributed under the LGPL License at: http://transcriptome.ens.fr/eoulsan/},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/bts165},
  Doi                      = {10.1093/bioinformatics/bts165},
  Institution              = {{\ldots}cole normale sup{\`E}rieure, Institut de Biologie de l'ENS, INSERM U1024, Paris, France. eoulsan@biologie.ens.fr},
  Keywords                 = {Algorithms; Animals; Computational Biology, methods; High-Throughput Nucleotide Sequencing, methods; Mice; Sequence Analysis, RNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {bts165},
  Pmid                     = {22492314},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/bts165}
}

@Article{Ko2014,
  Title                    = {Predicting the severity of motor neuron disease progression using electronic health record data with a cloud computing Big Data approach.},
  Author                   = {Ko, Kyung Dae and El-Ghazawi, Tarek and Kim, Dongkyu and Morizono, Hiroki and , Pooled Resource Open-Access A. L. S Clinical Trials Consortium},
  Journal                  = {IEEE Symp Comput Intell Bioinforma Comput Biol Proc},
  Year                     = {2014},

  Month                    = {May},
  Volume                   = {2014},
  Abstract                 = {Motor neuron diseases (MNDs) are a class of progressive neurological diseases that damage the motor neurons. An accurate diagnosis is important for the treatment of patients with MNDs because there is no standard cure for the MNDs. However, the rates of false positive and false negative diagnoses are still very high in this class of diseases. In the case of Amyotrophic Lateral Sclerosis (ALS), current estimates indicate 10\% of diagnoses are false-positives, while 44\% appear to be false negatives. In this study, we developed a new methodology to profile specific medical information from patient medical records for predicting the progression of motor neuron diseases. We implemented a system using Hbase and the Random forest classifier of Apache Mahout to profile medical records provided by the Pooled Resource Open-Access ALS Clinical Trials Database (PRO-ACT) site, and we achieved 66\% accuracy in the prediction of ALS progress.},
  Institution              = {Center for Genetic Medicine, Children's National Medical Center, Washington DC, United States.},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pmid                     = {25580472},
  Timestamp                = {2015.08.14}
}

@Article{Laitman2013,
  Title                    = {Haplotype analysis of the 185delAG BRCA1 mutation in ethnically diverse populations.},
  Author                   = {Laitman, Yael and Feng, Bing-Jian and Zamir, Itay M. and Weitzel, Jeffrey N. and Duncan, Paul and Port, Danielle and Thirthagiri, Eswary and Teo, Soo-Hwang and Evans, Gareth and Latif, Ayse and Newman, William G. and Gershoni-Baruch, Ruth and Zidan, Jamal and Shimon-Paluch, Shani and Goldgar, David and Friedman, Eitan},
  Journal                  = {Eur J Hum Genet},
  Year                     = {2013},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {212--216},
  Volume                   = {21},

  Abstract                 = {The 185delAG* BRCA1 mutation is encountered primarily in Jewish Ashkenazi and Iraqi individuals, and sporadically in non-Jews. Previous studies estimated that this is a founder mutation in Jewish mutation carriers that arose before the dispersion of Jews in the Diaspora ~2500 years ago. The aim of this study was to assess the haplotype in ethnically diverse 185delAG* BRCA1 mutation carriers, and to estimate the age at which the mutation arose. Ethnically diverse Jewish and non-Jewish 185delAG*BRCA1 mutation carriers and their relatives were genotyped using 15 microsatellite markers and three SNPs spanning 12.5?MB, encompassing the BRCA1 gene locus. Estimation of mutation age was based on a subset of 11 markers spanning a region of ~5?MB, using a previously developed algorithm applying the maximum likelihood method. Overall, 188 participants (154 carriers and 34 noncarriers) from 115 families were included: Ashkenazi, Iraq, Kuchin-Indians, Syria, Turkey, Iran, Tunisia, Bulgaria, non-Jewish English, non-Jewish Malaysian, and Hispanics. Haplotype analysis indicated that the 185delAG mutation arose 750-1500 years ago. In Ashkenazim, it is a founder mutation that arose 61 generations ago, and with a small group of founder mutations was introduced into the Hispanic population (conversos) ~650 years ago, and into the Iraqi-Jewish community ~450 years ago. The 185delAG mutation in the non-Jewish populations in Malaysia and the UK arose at least twice independently. We conclude that the 185delAG* BRCA1 mutation resides on a common haplotype among Ashkenazi Jews, and arose about 61 generations ago and arose independently at least twice in non-Jews.},
  Bdsk-url-1               = {http://dx.doi.org/10.1038/ejhg.2012.124},
  Doi                      = {10.1038/ejhg.2012.124},
  Institution              = {The Susanne Levy Gertner Oncogenetics Unit, The Danek Gertner Institute of Human Genetics, Chaim Sheba Medical Center, Tel-Hashomer, Israel.},
  Keywords                 = {BRCA1 Protein, genetics; Ethnic Groups, genetics; Founder Effect; Genetics, Population; Haplotypes; Humans; Jews, genetics; Sequence Deletion},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {ejhg2012124},
  Pmid                     = {22763381},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/ejhg.2012.124}
}

@Article{Langmead2009,
  Title                    = {Searching for SNPs with cloud computing.},
  Author                   = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai and Salzberg, Steven L.},
  Journal                  = {Genome Biol},
  Year                     = {2009},
  Number                   = {11},
  Pages                    = {R134},
  Volume                   = {10},

  Abstract                 = {As DNA sequencing outpaces improvements in computer speed, there is a critical need to accelerate tasks like alignment and SNP calling. Crossbow is a cloud-computing software tool that combines the aligner Bowtie and the SNP caller SOAPsnp. Executing in parallel using Hadoop, Crossbow analyzes data comprising 38-fold coverage of the human genome in three hours using a 320-CPU cluster rented from a cloud computing service for about $85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/gb-2009-10-11-r134},
  Doi                      = {10.1186/gb-2009-10-11-r134},
  Institution              = {Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, 615 North Wolfe Street, Baltimore, Maryland 21205, USA. blangmea@jhsph.edu},
  Keywords                 = {Algorithms; Alleles; Chromosomes, Human, Pair 22, genetics; Chromosomes, Human, X, genetics; Chromosomes, ultrastructure; Computational Biology, methods; Computer Simulation; Computers; Heterozygote; Humans; Models, Genetic; Polymorphism, Single Nucleotide; Sequence Analysis, DNA; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gb-2009-10-11-r134},
  Pmid                     = {19930550},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/gb-2009-10-11-r134}
}

@Article{Lunshof2013,
  Title                    = {Our genomes today: time to be clear.},
  Author                   = {Lunshof, Jeantine E. and Ball, Madeleine P.},
  Journal                  = {Genome Med},
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {52},
  Volume                   = {5},

  __markedentry            = {[bau04c:]},
  Doi                      = {10.1186/gm456},
  Institution              = {Department of Genetics, Harvard Medical School, 77 Avenue Louis Pasteur, Boston, MA 02215, USA.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {gm456},
  Pmid                     = {23806003},
  Timestamp                = {2015.08.14},
  Url                      = {http://dx.doi.org/10.1186/gm456}
}

@TechReport{Massie2013,
  Title                    = {ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing},
  Author                   = {Massie, Matt and Nothaft, Frank and Hartl, Christopher and Kozanitis, Christos and Schumacher, Andr{\`E} and Joseph, Anthony D. and Patterson, David A.},
  Institution              = {EECS Department, University of California, Berkeley},
  Year                     = {2013},
  Month                    = {Dec},
  Number                   = {UCB/EECS-2013-207},

  Bdsk-url-1               = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html},
  Url                      = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html}
}

@Article{McKenna2010,
  Title                    = {The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data.},
  Author                   = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and DePristo, Mark A.},
  Journal                  = {Genome Res},
  Year                     = {2010},

  Month                    = {Sep},
  Number                   = {9},
  Pages                    = {1297--1303},
  Volume                   = {20},

  Abstract                 = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS--the 1000 Genome pilot alone includes nearly five terabases--make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide polymorphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
  Bdsk-url-1               = {http://dx.doi.org/10.1101/gr.107524.110},
  Doi                      = {10.1101/gr.107524.110},
  Institution              = {Program in Medical and Population Genetics, The Broad Institute of Harvard and MIT, Cambridge, Massachusetts 02142, USA.},
  Keywords                 = {Base Sequence; Genome; Genomics, methods; Sequence Analysis, DNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gr.107524.110},
  Pmid                     = {20644199},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1101/gr.107524.110}
}

@Article{Nordberg2013,
  Title                    = {BioPig: a Hadoop-based analytic toolkit for large-scale sequence data.},
  Author                   = {Nordberg, Henrik and Bhatia, Karan and Wang, Kai and Wang, Zhong},
  Journal                  = {Bioinformatics},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {23},
  Pages                    = {3014--3019},
  Volume                   = {29},

  Abstract                 = {The recent revolution in sequencing technologies has led to an exponential growth of sequence data. As a result, most of the current bioinformatics tools become obsolete as they fail to scale with data. To tackle this 'data deluge', here we introduce the BioPig sequence analysis toolkit as one of the solutions that scale to data and computation.We built BioPig on the Apache's Hadoop MapReduce system and the Pig data flow language. Compared with traditional serial and MPI-based algorithms, BioPig has three major advantages: first, BioPig's programmability greatly reduces development time for parallel bioinformatics applications; second, testing BioPig with up to 500 Gb sequences demonstrates that it scales automatically with size of data; and finally, BioPig can be ported without modification on many Hadoop infrastructures, as tested with Magellan system at National Energy Research Scientific Computing Center and the Amazon Elastic Compute Cloud. In summary, BioPig represents a novel program framework with the potential to greatly accelerate data-intensive bioinformatics analysis.},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btt528},
  Doi                      = {10.1093/bioinformatics/btt528},
  Institution              = {Department of Energy, Joint Genome Institute, Walnut Creek, CA 94598, USA and Genomics Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA.},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btt528},
  Pmid                     = {24021384},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btt528}
}

@Article{OConnor2010,
  Title                    = {SeqWare Query Engine: storing and searching sequence data in the cloud.},
  Author                   = {O'Connor, Brian D. and Merriman, Barry and Nelson, Stanley F.},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S2},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Since the introduction of next-generation DNA sequencers the rapid increase in sequencer throughput, and associated drop in costs, has resulted in more than a dozen human genomes being resequenced over the last few years. These efforts are merely a prelude for a future in which genome resequencing will be commonplace for both biomedical research and clinical applications. The dramatic increase in sequencer output strains all facets of computational infrastructure, especially databases and query interfaces. The advent of cloud computing, and a variety of powerful tools designed to process petascale datasets, provide a compelling solution to these ever increasing demands.In this work, we present the SeqWare Query Engine which has been created using modern cloud computing technologies and designed to support databasing information from thousands of genomes. Our backend implementation was built using the highly scalable, NoSQL HBase database from the Hadoop project. We also created a web-based frontend that provides both a programmatic and interactive query interface and integrates with widely used genome browsers and tools. Using the query engine, users can load and query variants (SNVs, indels, translocations, etc) with a rich level of annotations including coverage and functional consequences. As a proof of concept we loaded several whole genome datasets including the U87MG cell line. We also used a glioblastoma multiforme tumor/normal pair to both profile performance and provide an example of using the Hadoop MapReduce framework within the query engine. This software is open source and freely available from the SeqWare project (http://seqware.sourceforge.net).The SeqWare Query Engine provided an easy way to make the U87MG genome accessible to programmers and non-programmers alike. This enabled a faster and more open exploration of results, quicker tuning of parameters for heuristic variant calling filters, and a common data interface to simplify development of analytical tools. The range of data types supported, the ease of querying and integrating with existing tools, and the robust scalability of the underlying cloud-based technologies make SeqWare Query Engine a nature fit for storing and searching ever-growing genome sequence datasets.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2},
  Doi                      = {10.1186/1471-2105-11-S12-S2},
  Institution              = {UNC Lineberger Comprehensive Cancer Center, University of North Carolina, Chapel Hill, NC 27599, USA.},
  Keywords                 = {Databases, Nucleic Acid; Genome, Human; Genomics, methods; High-Throughput Nucleotide Sequencing; Humans; Sequence Analysis, DNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S2},
  Pmid                     = {21210981},
  Timestamp                = {2014.01.08},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2}
}

@Book{Owen2011,
  Title                    = {Mahout in Action},
  Author                   = {Owen, Sean and Anil, Robin and Dunning, Ted and Friedman, Ellen},
  Publisher                = {Manning Publications Co.},
  Year                     = {2011},

  Address                  = {Manning Publications Co. 20 Baldwin Road PO Box 261 Shelter Island, NY 11964},
  Edition                  = {First},

  Abstract                 = {This book covers machine learning using Apache Mahout. Based on experience with real-world applications, it introduces practical use cases and illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability and how to apply these techniques against large data sets using the Apache Hadoop framework. This book is written for developers familiar with Java. No prior experience with Mahout is assumed.},
  Added-at                 = {2012-03-07T16:04:47.000+0100},
  Bdsk-url-1               = {http://manning.com/owen/},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2f6d421df62439bfda65253c3d8eebab7/telekoma},
  Interhash                = {a1eacf0ec14439d4042a2a7d9515f697},
  Intrahash                = {f6d421df62439bfda65253c3d8eebab7},
  Keywords                 = {action bachelor:2011:bachmann mahout},
  Timestamp                = {2012-03-07T16:04:47.000+0100},
  Url                      = {http://manning.com/owen/}
}

@Article{Qiu2010,
  Title                    = {Hybrid cloud and cluster computing paradigms for life science applications.},
  Author                   = {Qiu, Judy and Ekanayake, Jaliya and Gunarathne, Thilina and Choi, Jong Youl and Bae, Seung-Hee and Li, Hui and Zhang, Bingjing and Wu, Tak-Lon and Ruan, Yang and Ekanayake, Saliya and Hughes, Adam and Fox, Geoffrey},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S3},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Clouds and MapReduce have shown themselves to be a broadly useful approach to scientific computing especially for parallel data intensive applications. However they have limited applicability to some areas such as data mining because MapReduce has poor performance on problems with an iterative structure present in the linear algebra that underlies much data analysis. Such problems can be run efficiently on clusters using MPI leading to a hybrid cloud and cluster environment. This motivates the design and implementation of an open source Iterative MapReduce system Twister.Comparisons of Amazon, Azure, and traditional Linux and Windows environments on common applications have shown encouraging performance and usability comparisons in several important non iterative cases. These are linked to MPI applications for final stages of the data analysis. Further we have released the open source Twister Iterative MapReduce and benchmarked it against basic MapReduce (Hadoop) and MPI in information retrieval and life sciences applications.The hybrid cloud (MapReduce) and cluster (MPI) approach offers an attractive production environment while Twister promises a uniform programming environment for many Life Sciences applications.We used commercial clouds Amazon and Azure and the NSF resource FutureGrid to perform detailed comparisons and evaluations of different approaches to data intensive computing. Several applications were developed in MPI, MapReduce and Twister in these different environments.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S3},
  Doi                      = {10.1186/1471-2105-11-S12-S3},
  Institution              = {School of Informatics and Computing, Indiana University, Bloomington, IN 47405, USA. xqiu@indiana.edu},
  Keywords                 = {Biological Science Disciplines; Cluster Analysis; Computational Biology, methods; Data Mining; Metagenomics; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S3},
  Pmid                     = {21210982},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S3}
}

@InProceedings{Ranger2007,
  Title                    = {Evaluating MapReduce for Multi-core and Multiprocessor Systems},
  Author                   = {Ranger, C. and Raghuraman, R. and Penmetsa, A. and Bradski, G. and Kozyrakis, C.},
  Booktitle                = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
  Year                     = {2007},
  Month                    = {Feb},
  Pages                    = {13-24},

  Bdsk-url-1               = {http://dx.doi.org/10.1109/HPCA.2007.346181},
  Doi                      = {10.1109/HPCA.2007.346181},
  Keywords                 = {fault tolerance;multi-threading;multiprocessing systems;performance evaluation;scheduling;MapReduce;Phoenix;data partitioning;distributed system;dynamic task scheduling;error recovery;fault tolerance;functional-style code;multicore system;multiprocessor system;programming API and;shared-memory systems;thread creation;Concurrent computing;Dynamic scheduling;Fault tolerance;Laboratories;Multiprocessing systems;Parallel programming;Processor scheduling;Programming profession;Runtime;Yarn}
}

@Article{Schatz2009,
  Title                    = {CloudBurst: highly sensitive read mapping with MapReduce.},
  Author                   = {Schatz, Michael C.},
  Journal                  = {Bioinformatics},
  Year                     = {2009},

  Month                    = {Jun},
  Number                   = {11},
  Pages                    = {1363--1369},
  Volume                   = {25},

  Abstract                 = {Next-generation DNA sequencing machines are generating an enormous amount of sequence data, placing unprecedented demands on traditional single-processor read-mapping algorithms. CloudBurst is a new parallel read-mapping algorithm optimized for mapping next-generation sequence data to the human genome and other reference genomes, for use in a variety of biological analyses including SNP discovery, genotyping and personal genomics. It is modeled after the short read-mapping program RMAP, and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences. This level of sensitivity could be prohibitively time consuming, but CloudBurst uses the open-source Hadoop implementation of MapReduce to parallelize execution using multiple compute nodes.CloudBurst's running time scales linearly with the number of reads mapped, and with near linear speedup as the number of processors increases. In a 24-processor core configuration, CloudBurst is up to 30 times faster than RMAP executing on a single core, while computing an identical set of alignments. Using a larger remote compute cloud with 96 cores, CloudBurst improved performance by >100-fold, reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome.CloudBurst is available open-source as a model for parallelizing algorithms with MapReduce at (http://cloudburst-bio.sourceforge.net/).},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btp236},
  Doi                      = {10.1093/bioinformatics/btp236},
  Institution              = {Center for Bioinformatics and Computational Biology, University of Maryland, College Park, MD 20742, USA. mschatz@umiacs.umd.edu},
  Keywords                 = {Algorithms; Animals; Computational Biology, methods; DNA; Genome; Humans; Internet; Sequence Alignment; Sequence Analysis, DNA, methods},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btp236},
  Pmid                     = {19357099},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btp236}
}

@Article{Schumacher2014,
  Title                    = {SeqPig: simple and scalable scripting for large sequencing data sets in Hadoop.},
  Author                   = {Schumacher, Andr{\`E} and Pireddu, Luca and Niemenmaa, Matti and Kallio, Aleksi and Korpelainen, Eija and Zanetti, Gianluigi and Heljanko, Keijo},
  Journal                  = {Bioinformatics},
  Year                     = {2014},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {119--120},
  Volume                   = {30},

  Abstract                 = {Hadoop MapReduce-based approaches have become increasingly popular due to their scalability in processing large sequencing datasets. However, as these methods typically require in-depth expertise in Hadoop and Java, they are still out of reach of many bioinformaticians. To solve this problem, we have created SeqPig, a library and a collection of tools to manipulate, analyze and query sequencing datasets in a scalable and simple manner. SeqPigscripts use the Hadoop-based distributed scripting engine Apache Pig, which automatically parallelizes and distributes data processing tasks. We demonstrate SeqPig's scalability over many computing nodes and illustrate its use with example scripts.Available under the open source MIT license at http://sourceforge.net/projects/seqpig/},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btt601},
  Doi                      = {10.1093/bioinformatics/btt601},
  Institution              = {Aalto University School of Science and Helsinki Institute for Information Technology HIIT, Finland, International Computer Science Institute, Berkeley, CA, USA, CRS4-Center for Advanced Studies, Research and Development in Sardinia, Italy and CSC-IT Center for Science, Finland.},
  Keywords                 = {High-Throughput Screening Assays, methods; Software Design},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btt601},
  Pmid                     = {24149054},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btt601}
}

@Article{Stein2010,
  Title                    = {The case for cloud computing in genome informatics.},
  Author                   = {Stein, Lincoln D.},
  Journal                  = {Genome Biol},
  Year                     = {2010},
  Number                   = {5},
  Pages                    = {207},
  Volume                   = {11},

  Abstract                 = {With DNA sequencing now getting cheaper more quickly than data storage or computation, the time may have come for genome informatics to migrate to the cloud.},
  Doi                      = {10.1186/gb-2010-11-5-207},
  Institution              = {Ontario Institute for Cancer Research, Toronto, ON M5G 0A3, Canada. lincoln.stein@gmail.com},
  Keywords                 = {Computational Biology, economics/methods; Genome, Human, genetics; Humans; Sequence Analysis, DNA, economics/methods},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gb-2010-11-5-207},
  Pmid                     = {20441614},
  Timestamp                = {2015.08.14},
  Url                      = {http://dx.doi.org/10.1186/gb-2010-11-5-207}
}

@Article{Taylor2010,
  Title                    = {An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics.},
  Author                   = {Taylor, Ronald C.},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S1},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Bioinformatics researchers are now confronted with analysis of ultra large-scale data sets, a problem that will only increase at an alarming rate in coming years. Recent developments in open source software, that is, the Hadoop project and associated software, provide a foundation for scaling to petabyte scale data warehouses on Linux clusters, providing fault-tolerant parallelized analysis on such data using a programming style named MapReduce.An overview is given of the current usage within the bioinformatics community of Hadoop, a top-level Apache Software Foundation project, and of associated open source software projects. The concepts behind Hadoop and the associated HBase project are defined, and current bioinformatics software that employ Hadoop is described. The focus is on next-generation sequencing, as the leading application area to date.Hadoop and the MapReduce programming paradigm already have a substantial base in the bioinformatics community, especially in the field of next-generation sequencing analysis, and such use is increasing. This is due to the cost-effectiveness of Hadoop-based analysis on commodity Linux clusters, and in the cloud via data upload to cloud vendors who have implemented Hadoop/HBase; and due to the effectiveness and ease-of-use of the MapReduce method in parallelization of many data analysis algorithms.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S1},
  Doi                      = {10.1186/1471-2105-11-S12-S1},
  Institution              = {Computational Biology and Bioinformatics Group, Pacific Northwest National Laboratory, Richland, Washington 99352, USA. ronald.taylor@pnl.gov},
  Keywords                 = {Algorithms; Cluster Analysis; Computational Biology, methods; High-Throughput Nucleotide Sequencing; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S1},
  Pmid                     = {21210976},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S1}
}

@Other{Zaharia2011,
  Title                    = {Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing},
  Abstract                 = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks. 1},
  Author                   = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and Mccauley, Murphy and Michael J. Franklin and Shenker, Scott and Stoica, Ion},
  Owner                    = {bau04c},
  Timestamp                = {2015.08.14},
  Year                     = {2011}
}

@Article{Zou2013,
  Title                    = {Survey of MapReduce frame operation in bioinformatics.},
  Author                   = {Zou, Quan and Li, Xu-Bin and Jiang, Wen-Rui and Lin, Zi-Yu and Li, Gui-Lin and Chen, Ke},
  Journal                  = {Brief Bioinform},
  Year                     = {2013},

  Month                    = {Feb},

  Abstract                 = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bib/bbs088},
  Doi                      = {10.1093/bib/bbs088},
  Language                 = {eng},
  Medline-pst              = {aheadofprint},
  Owner                    = {bau04c},
  Pii                      = {bbs088},
  Pmid                     = {23396756},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bib/bbs088}
}

@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

