% This file was created with JabRef 2.8.
% Encoding: ISO8859_1

@ARTICLE{1KG2012,
  author = {{1000 Genomes Project Consortium}},
  title = {An integrated map of genetic variation from 1,092 human genomes.},
  journal = {Nature},
  year = {2012},
  volume = {491},
  pages = {56--65},
  number = {7422},
  month = {Nov},
  doi = {10.1038/nature11632},
  keywords = {Alleles; Binding Sites, genetics; Conserved Sequence, genetics; Continental
	Population Groups, genetics; Evolution, Molecular; Genetic Variation,
	genetics; Genetics, Medical; Genetics, Population; Genome, Human,
	genetics; Genome-Wide Association Study; Genomics; Haplotypes, genetics;
	Humans; Nucleotide Motifs; Polymorphism, Single Nucleotide, genetics;
	Sequence Deletion, genetics; Transcription Factors, metabolism},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {nature11632},
  pmid = {23128226},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1038/nature11632}
}

@ARTICLE{TCGA2013,
  author = {{Cancer Genome Atlas Research Network}},
  title = {The Cancer Genome Atlas Pan-Cancer analysis project.},
  journal = {Nat Genet},
  year = {2013},
  volume = {45},
  pages = {1113--1120},
  number = {10},
  month = {Oct},
  __markedentry = {[bau04c:]},
  abstract = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed
	large numbers of human tumors to discover molecular aberrations at
	the DNA, RNA, protein and epigenetic levels. The resulting rich data
	provide a major opportunity to develop an integrated picture of commonalities,
	differences and emergent themes across tumor lineages. The Pan-Cancer
	initiative compares the first 12 tumor types profiled by TCGA. Analysis
	of the molecular aberrations and their functional roles across tumor
	types will teach us how to extend therapies effective in one cancer
	type to others with a similar genomic profile.},
  doi = {10.1038/ng.2764},
  keywords = {Gene Expression Profiling; Genome; Humans; Neoplasms, genetics/pathology},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {ng.2764},
  pmid = {24071849},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1038/ng.2764}
}

@INPROCEEDINGS{Chu2009,
  author = {Chu, Cheng T. and Kim, Sang K. and Lin, Yi A. and Yu, Yuanyuan and
	Bradski, Gary R. and Ng, Andrew Y. and Olukotun, Kunle},
  title = {Map-Reduce for Machine Learning on Multicore},
  booktitle = {NIPS},
  year = {2006},
  editor = {Sch\"{o}lkopf, Bernhard and Platt, John C. and Hoffman, Thomas},
  pages = {281--288},
  publisher = {MIT Press},
  added-at = {2009-08-17T16:24:20.000+0200},
  biburl = {http://www.bibsonomy.org/bibtex/20de668cfdfc349fe197f30c7d840ba0f/sb3000},
  citeulike-article-id = {2308503},
  citeulike-linkout-0 = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06},
  citeulike-linkout-1 = {http://www.cs.stanford.edu/people/ang//papers/nips06-mapreducemulticore.pdf},
  description = {CiteULike: Map-Reduce for Machine Learning on Multicore},
  interhash = {96ced10b0cf1c9ce041cf2b43574656c},
  intrahash = {0de668cfdfc349fe197f30c7d840ba0f},
  keywords = {cloud mapreduce ml},
  posted-at = {2008-03-07 03:16:12},
  priority = {0},
  timestamp = {2009-08-17T16:24:20.000+0200},
  url = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06}
}

@ARTICLE{Doering2008,
  author = {D{\"o}ring, Andreas and Weese, David and Rausch, Tobias and Reinert,
	Knut},
  title = {SeqAn an efficient, generic C++ library for sequence analysis.},
  journal = {BMC Bioinformatics},
  year = {2008},
  volume = {9},
  pages = {11},
  abstract = {The use of novel algorithmic techniques is pivotal to many important
	problems in life science. For example the sequencing of the human
	genome 1 would not have been possible without advanced assembly algorithms.
	However, owing to the high speed of technological progress and the
	urgent need for bioinformatics tools, there is a widening gap between
	state-of-the-art algorithmic techniques and the actual algorithmic
	components of tools that are in widespread use.To remedy this trend
	we propose the use of SeqAn, a library of efficient data types and
	algorithms for sequence analysis in computational biology. SeqAn
	comprises implementations of existing, practical state-of-the-art
	algorithmic components to provide a sound basis for algorithm testing
	and development. In this paper we describe the design and content
	of SeqAn and demonstrate its use by giving two examples. In the first
	example we show an application of SeqAn as an experimental platform
	by comparing different exact string matching algorithms. The second
	example is a simple version of the well-known MUMmer tool rewritten
	in SeqAn. Results indicate that our implementation is very efficient
	and versatile to use.We anticipate that SeqAn greatly simplifies
	the rapid development of new bioinformatics tools by providing a
	collection of readily usable, well-designed algorithmic components
	which are fundamental for the field of sequence analysis. This leverages
	not only the implementation of new algorithms, but also enables a
	sound analysis and comparison of existing algorithms.},
  doi = {10.1186/1471-2105-9-11},
  institution = {Algorithmische Bioinformatik, Institut für Informatik, Takustr, 9,
	14195 Berlin, Germany. doering@inf.fu-berlin.de},
  keywords = {Algorithms; Database Management Systems; Databases, Genetic; Programming
	Languages; Sequence Alignment, methods; Sequence Analysis, methods;
	Software; User-Computer Interface},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-9-11},
  pmid = {18184432},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/1471-2105-9-11}
}

@ARTICLE{Dong2013,
  author = {Dong, Xiao and Bahroos, Neil and Sadhu, Eugene and Jackson, Tommie
	and Chukhman, Morris and Johnson, Robert and Boyd, Andrew and Hynes,
	Denise},
  title = {Leverage hadoop framework for large scale clinical informatics applications.},
  journal = {AMIA Summits Transl Sci Proc},
  year = {2013},
  volume = {2013},
  pages = {53},
  institution = {University of Illinois at Chicago, Chicago, IL.},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pmid = {24303235},
  timestamp = {2014.05.01}
}

@ARTICLE{Gao2007,
  author = {Gao, Xiaoyi and Starmer, Joshua},
  title = {Human population structure detection via multilocus genotype clustering.},
  journal = {BMC Genet},
  year = {2007},
  volume = {8},
  pages = {34},
  abstract = {We describe a hierarchical clustering algorithm for using Single Nucleotide
	Polymorphism (SNP) genetic data to assign individuals to populations.
	The method does not assume Hardy-Weinberg equilibrium and linkage
	equilibrium among loci in sample population individuals.We show that
	the algorithm can assign sample individuals highly accurately to
	their corresponding ethnic groups in our tests using HapMap SNP data
	and it is also robust to admixed populations when tested with Perlegen
	SNP data. Moreover, it can detect fine-scale population structure
	as subtle as that between Chinese and Japanese by using genome-wide
	high-diversity SNP loci.The algorithm provides an alternative approach
	to the popular STRUCTURE program, especially for fine-scale population
	structure detection in genome-wide association studies. This is the
	first successful separation of Chinese and Japanese samples using
	random SNP loci with high statistical support.},
  doi = {10.1186/1471-2156-8-34},
  institution = {Miami Institute for Human Genomics, University of Miami Miller School
	of Medicine, Miami, FL 33136, USA. xgao@med.miami.edu},
  keywords = {Algorithms; Asian Continental Ancestry Group, classification/genetics;
	Cluster Analysis; Computer Simulation; Gene Frequency; Genetic Markers;
	Genetics, Population; Genotype; Humans; Models, Genetic; Polymorphism,
	Single Nucleotide},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2156-8-34},
  pmid = {17592628},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/1471-2156-8-34}
}

@ARTICLE{Guo2014,
  author = {Guo, Xuan and Meng, Yu and Yu, Ning and Pan, Yi},
  title = {Cloud computing for detecting high-order genome-wide epistatic interaction
	via dynamic clustering.},
  journal = {BMC Bioinformatics},
  year = {2014},
  volume = {15},
  pages = {102},
  number = {1},
  abstract = {Taking the advan tage of high-throughput single nucleotide polymorphism
	(SNP) genotyping technology, large genome-wide association studies
	(GWASs) have been considered to hold promise for unravelling complex
	relationships between genotype and phenotype. At present, traditional
	single-locus-based methods are insufficient to detect interactions
	consisting of multiple-locus, which are broadly existing in complex
	traits. In addition, statistic tests for high order epistatic interactions
	with more than 2 SNPs propose computational and analytical challenges
	because the computation increases exponentially as the cardinality
	of SNPs combinations gets larger.In this paper, we provide a simple,
	fast and powerful method using dynamic clustering and cloud computing
	to detect genome-wide multi-locus epistatic interactions. We have
	constructed systematic experiments to compare powers performance
	against some recently proposed algorithms, including TEAM, SNPRuler,
	EDCF and BOOST. Furthermore, we have applied our method on two real
	GWAS datasets, Age-related macular degeneration (AMD) and Rheumatoid
	arthritis (RA) datasets, where we find some novel potential disease-related
	genetic factors which are not shown up in detections of 2-loci epistatic
	interactions.Experimental results on simulated data demonstrate that
	our method is more powerful than some recently proposed methods on
	both two- and three-locus disease models. Our method has discovered
	many novel high-order associations that are significantly enriched
	in cases from two real GWAS datasets. Moreover, the running time
	of the cloud implementation for our method on AMD dataset and RA
	dataset are roughly 2 hours and 50 hours on a cluster with forty
	small virtual machines for detecting two-locus interactions, respectively.
	Therefore, we believe that our method is suitable and effective for
	the full-scale analysis of multiple-locus epistatic interactions
	in GWAS.},
  doi = {10.1186/1471-2105-15-102},
  institution = {Department of Computer Science, Georgia State University, 34 Peachtree
	Street, Atlanta, USA. yipan@gsu.edu.},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-15-102},
  pmid = {24717145},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/1471-2105-15-102}
}

@ARTICLE{Huang2013,
  author = {Huang, Hailiang and Tata, Sandeep and Prill, Robert J.},
  title = {BlueSNP: R package for highly scalable genome-wide association studies
	using Hadoop clusters.},
  journal = {Bioinformatics},
  year = {2013},
  volume = {29},
  pages = {135--136},
  number = {1},
  month = {Jan},
  abstract = {Computational workloads for genome-wide association studies (GWAS)
	are growing in scale and complexity outpacing the capabilities of
	single-threaded software designed for personal computers. The BlueSNP
	R package implements GWAS statistical tests in the R programming
	language and executes the calculations across computer clusters configured
	with Apache Hadoop, a de facto standard framework for distributed
	data processing using the MapReduce formalism. BlueSNP makes computationally
	intensive analyses, such as estimating empirical p-values via data
	permutation, and searching for expression quantitative trait loci
	over thousands of genes, feasible for large genotype-phenotype datasets.
	Availability and implementation: http://github.com/ibm-bioinformatics/bluesnp},
  doi = {10.1093/bioinformatics/bts647},
  institution = {Healthcare Informatics, IBM Almaden Research Center, San Jose, CA
	95120, USA.},
  keywords = {Genome-Wide Association Study; Humans; Phenotype; Quantitative Trait
	Loci; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts647},
  pmid = {23202745},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts647}
}

@ARTICLE{Jourdren2012,
  author = {Jourdren, Laurent and Bernard, Maria and Dillies, Marie-Agnès and
	{Le Crom}, Stéphane},
  title = {Eoulsan: a cloud computing-based framework facilitating high throughput
	sequencing analyses.},
  journal = {Bioinformatics},
  year = {2012},
  volume = {28},
  pages = {1542--1543},
  number = {11},
  month = {Jun},
  abstract = {We developed a modular and scalable framework called Eoulsan, based
	on the Hadoop implementation of the MapReduce algorithm dedicated
	to high-throughput sequencing data analysis. Eoulsan allows users
	to easily set up a cloud computing cluster and automate the analysis
	of several samples at once using various software solutions available.
	Our tests with Amazon Web Services demonstrated that the computation
	cost is linear with the number of instances booked as is the running
	time with the increasing amounts of data. Availability and implementation:
	Eoulsan is implemented in Java, supported on Linux systems and distributed
	under the LGPL License at: http://transcriptome.ens.fr/eoulsan/},
  doi = {10.1093/bioinformatics/bts165},
  institution = {École normale supérieure, Institut de Biologie de l'ENS, INSERM U1024,
	Paris, France. eoulsan@biologie.ens.fr},
  keywords = {Algorithms; Animals; Computational Biology, methods; High-Throughput
	Nucleotide Sequencing, methods; Mice; Sequence Analysis, RNA, methods;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {bts165},
  pmid = {22492314},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts165}
}

@ARTICLE{Laitman2013,
  author = {Laitman, Yael and Feng, Bing-Jian and Zamir, Itay M. and Weitzel,
	Jeffrey N. and Duncan, Paul and Port, Danielle and Thirthagiri, Eswary
	and Teo, Soo-Hwang and Evans, Gareth and Latif, Ayse and Newman,
	William G. and Gershoni-Baruch, Ruth and Zidan, Jamal and Shimon-Paluch,
	Shani and Goldgar, David and Friedman, Eitan},
  title = {Haplotype analysis of the 185delAG BRCA1 mutation in ethnically diverse
	populations.},
  journal = {Eur J Hum Genet},
  year = {2013},
  volume = {21},
  pages = {212--216},
  number = {2},
  month = {Feb},
  __markedentry = {[bau04c:6]},
  abstract = {The 185delAG* BRCA1 mutation is encountered primarily in Jewish Ashkenazi
	and Iraqi individuals, and sporadically in non-Jews. Previous studies
	estimated that this is a founder mutation in Jewish mutation carriers
	that arose before the dispersion of Jews in the Diaspora ~2500 years
	ago. The aim of this study was to assess the haplotype in ethnically
	diverse 185delAG* BRCA1 mutation carriers, and to estimate the age
	at which the mutation arose. Ethnically diverse Jewish and non-Jewish
	185delAG*BRCA1 mutation carriers and their relatives were genotyped
	using 15 microsatellite markers and three SNPs spanning 12.5?MB,
	encompassing the BRCA1 gene locus. Estimation of mutation age was
	based on a subset of 11 markers spanning a region of ~5?MB, using
	a previously developed algorithm applying the maximum likelihood
	method. Overall, 188 participants (154 carriers and 34 noncarriers)
	from 115 families were included: Ashkenazi, Iraq, Kuchin-Indians,
	Syria, Turkey, Iran, Tunisia, Bulgaria, non-Jewish English, non-Jewish
	Malaysian, and Hispanics. Haplotype analysis indicated that the 185delAG
	mutation arose 750-1500 years ago. In Ashkenazim, it is a founder
	mutation that arose 61 generations ago, and with a small group of
	founder mutations was introduced into the Hispanic population (conversos)
	~650 years ago, and into the Iraqi-Jewish community ~450 years ago.
	The 185delAG mutation in the non-Jewish populations in Malaysia and
	the UK arose at least twice independently. We conclude that the 185delAG*
	BRCA1 mutation resides on a common haplotype among Ashkenazi Jews,
	and arose about 61 generations ago and arose independently at least
	twice in non-Jews.},
  doi = {10.1038/ejhg.2012.124},
  institution = {The Susanne Levy Gertner Oncogenetics Unit, The Danek Gertner Institute
	of Human Genetics, Chaim Sheba Medical Center, Tel-Hashomer, Israel.},
  keywords = {BRCA1 Protein, genetics; Ethnic Groups, genetics; Founder Effect;
	Genetics, Population; Haplotypes; Humans; Jews, genetics; Sequence
	Deletion},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {ejhg2012124},
  pmid = {22763381},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1038/ejhg.2012.124}
}

@ARTICLE{Langmead2009,
  author = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai
	and Salzberg, Steven L.},
  title = {Searching for SNPs with cloud computing.},
  journal = {Genome Biol},
  year = {2009},
  volume = {10},
  pages = {R134},
  number = {11},
  abstract = {As DNA sequencing outpaces improvements in computer speed, there is
	a critical need to accelerate tasks like alignment and SNP calling.
	Crossbow is a cloud-computing software tool that combines the aligner
	Bowtie and the SNP caller SOAPsnp. Executing in parallel using Hadoop,
	Crossbow analyzes data comprising 38-fold coverage of the human genome
	in three hours using a 320-CPU cluster rented from a cloud computing
	service for about $85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.},
  doi = {10.1186/gb-2009-10-11-r134},
  institution = {Department of Biostatistics, Johns Hopkins Bloomberg School of Public
	Health, 615 North Wolfe Street, Baltimore, Maryland 21205, USA. blangmea@jhsph.edu},
  keywords = {Algorithms; Alleles; Chromosomes, Human, Pair 22, genetics; Chromosomes,
	Human, X, genetics; Chromosomes, ultrastructure; Computational Biology,
	methods; Computer Simulation; Computers; Heterozygote; Humans; Models,
	Genetic; Polymorphism, Single Nucleotide; Sequence Analysis, DNA;
	Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gb-2009-10-11-r134},
  pmid = {19930550},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/gb-2009-10-11-r134}
}

@TECHREPORT{Massie2013,
  author = {Massie, Matt and Nothaft, Frank and Hartl, Christopher and Kozanitis,
	Christos and Schumacher, André and Joseph, Anthony D. and Patterson,
	David A.},
  title = {ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing},
  institution = {EECS Department, University of California, Berkeley},
  year = {2013},
  number = {UCB/EECS-2013-207},
  month = {Dec},
  url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html}
}

@ARTICLE{McKenna2010,
  author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko,
	Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella,
	Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and
	DePristo, Mark A.},
  title = {The Genome Analysis Toolkit: a MapReduce framework for analyzing
	next-generation DNA sequencing data.},
  journal = {Genome Res},
  year = {2010},
  volume = {20},
  pages = {1297--1303},
  number = {9},
  month = {Sep},
  abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes
	Project, are already revolutionizing our understanding of genetic
	variation among individuals. However, the massive data sets generated
	by NGS--the 1000 Genome pilot alone includes nearly five terabases--make
	writing feature-rich, efficient, and robust analysis tools difficult
	for even computationally sophisticated individuals. Indeed, many
	professionals are limited in the scope and the ease with which they
	can answer scientific questions by the complexity of accessing and
	manipulating the data produced by these machines. Here, we discuss
	our Genome Analysis Toolkit (GATK), a structured programming framework
	designed to ease the development of efficient and robust analysis
	tools for next-generation DNA sequencers using the functional programming
	philosophy of MapReduce. The GATK provides a small but rich set of
	data access patterns that encompass the majority of analysis tool
	needs. Separating specific analysis calculations from common data
	management infrastructure enables us to optimize the GATK framework
	for correctness, stability, and CPU and memory efficiency and to
	enable distributed and shared memory parallelization. We highlight
	the capabilities of the GATK by describing the implementation and
	application of robust, scale-tolerant tools like coverage calculators
	and single nucleotide polymorphism (SNP) calling. We conclude that
	the GATK programming framework enables developers and analysts to
	quickly and easily write efficient and robust NGS tools, many of
	which have already been incorporated into large-scale sequencing
	projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
  doi = {10.1101/gr.107524.110},
  institution = {Program in Medical and Population Genetics, The Broad Institute of
	Harvard and MIT, Cambridge, Massachusetts 02142, USA.},
  keywords = {Base Sequence; Genome; Genomics, methods; Sequence Analysis, DNA,
	methods; Software},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {gr.107524.110},
  pmid = {20644199},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1101/gr.107524.110}
}

@ARTICLE{Nordberg2013,
  author = {Nordberg, Henrik and Bhatia, Karan and Wang, Kai and Wang, Zhong},
  title = {BioPig: a Hadoop-based analytic toolkit for large-scale sequence
	data.},
  journal = {Bioinformatics},
  year = {2013},
  volume = {29},
  pages = {3014--3019},
  number = {23},
  month = {Dec},
  abstract = {The recent revolution in sequencing technologies has led to an exponential
	growth of sequence data. As a result, most of the current bioinformatics
	tools become obsolete as they fail to scale with data. To tackle
	this 'data deluge', here we introduce the BioPig sequence analysis
	toolkit as one of the solutions that scale to data and computation.We
	built BioPig on the Apache's Hadoop MapReduce system and the Pig
	data flow language. Compared with traditional serial and MPI-based
	algorithms, BioPig has three major advantages: first, BioPig's programmability
	greatly reduces development time for parallel bioinformatics applications;
	second, testing BioPig with up to 500 Gb sequences demonstrates that
	it scales automatically with size of data; and finally, BioPig can
	be ported without modification on many Hadoop infrastructures, as
	tested with Magellan system at National Energy Research Scientific
	Computing Center and the Amazon Elastic Compute Cloud. In summary,
	BioPig represents a novel program framework with the potential to
	greatly accelerate data-intensive bioinformatics analysis.},
  doi = {10.1093/bioinformatics/btt528},
  institution = {Department of Energy, Joint Genome Institute, Walnut Creek, CA 94598,
	USA and Genomics Division, Lawrence Berkeley National Laboratory,
	Berkeley, CA 94720, USA.},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btt528},
  pmid = {24021384},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bioinformatics/btt528}
}

@ARTICLE{OConnor2010,
  author = {O'Connor, Brian D. and Merriman, Barry and Nelson, Stanley F.},
  title = {SeqWare Query Engine: storing and searching sequence data in the
	cloud.},
  journal = {BMC Bioinformatics},
  year = {2010},
  volume = {11 Suppl 12},
  pages = {S2},
  abstract = {Since the introduction of next-generation DNA sequencers the rapid
	increase in sequencer throughput, and associated drop in costs, has
	resulted in more than a dozen human genomes being resequenced over
	the last few years. These efforts are merely a prelude for a future
	in which genome resequencing will be commonplace for both biomedical
	research and clinical applications. The dramatic increase in sequencer
	output strains all facets of computational infrastructure, especially
	databases and query interfaces. The advent of cloud computing, and
	a variety of powerful tools designed to process petascale datasets,
	provide a compelling solution to these ever increasing demands.In
	this work, we present the SeqWare Query Engine which has been created
	using modern cloud computing technologies and designed to support
	databasing information from thousands of genomes. Our backend implementation
	was built using the highly scalable, NoSQL HBase database from the
	Hadoop project. We also created a web-based frontend that provides
	both a programmatic and interactive query interface and integrates
	with widely used genome browsers and tools. Using the query engine,
	users can load and query variants (SNVs, indels, translocations,
	etc) with a rich level of annotations including coverage and functional
	consequences. As a proof of concept we loaded several whole genome
	datasets including the U87MG cell line. We also used a glioblastoma
	multiforme tumor/normal pair to both profile performance and provide
	an example of using the Hadoop MapReduce framework within the query
	engine. This software is open source and freely available from the
	SeqWare project (http://seqware.sourceforge.net).The SeqWare Query
	Engine provided an easy way to make the U87MG genome accessible to
	programmers and non-programmers alike. This enabled a faster and
	more open exploration of results, quicker tuning of parameters for
	heuristic variant calling filters, and a common data interface to
	simplify development of analytical tools. The range of data types
	supported, the ease of querying and integrating with existing tools,
	and the robust scalability of the underlying cloud-based technologies
	make SeqWare Query Engine a nature fit for storing and searching
	ever-growing genome sequence datasets.},
  doi = {10.1186/1471-2105-11-S12-S2},
  institution = {UNC Lineberger Comprehensive Cancer Center, University of North Carolina,
	Chapel Hill, NC 27599, USA.},
  keywords = {Databases, Nucleic Acid; Genome, Human; Genomics, methods; High-Throughput
	Nucleotide Sequencing; Humans; Sequence Analysis, DNA, methods; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-11-S12-S2},
  pmid = {21210981},
  timestamp = {2014.01.08},
  url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2}
}

@BOOK{Owen2011,
  title = {Mahout in Action},
  publisher = {Manning Publications Co.},
  year = {2011},
  author = {Owen, Sean and Anil, Robin and Dunning, Ted and Friedman, Ellen},
  address = {Manning Publications Co. 20 Baldwin Road PO Box 261 Shelter Island,
	NY 11964},
  edition = {First},
  abstract = {This book covers machine learning using Apache Mahout. Based on experience
	with real-world applications, it introduces practical use cases and
	illustrates how Mahout can be applied to solve them. It places particular
	focus on issues of scalability and how to apply these techniques
	against large data sets using the Apache Hadoop framework. This book
	is written for developers familiar with Java. No prior experience
	with Mahout is assumed.},
  added-at = {2012-03-07T16:04:47.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/2f6d421df62439bfda65253c3d8eebab7/telekoma},
  interhash = {a1eacf0ec14439d4042a2a7d9515f697},
  intrahash = {f6d421df62439bfda65253c3d8eebab7},
  keywords = {action bachelor:2011:bachmann mahout},
  timestamp = {2012-03-07T16:04:47.000+0100},
  url = {http://manning.com/owen/}
}

@ARTICLE{Qiu2010,
  author = {Qiu, Judy and Ekanayake, Jaliya and Gunarathne, Thilina and Choi,
	Jong Youl and Bae, Seung-Hee and Li, Hui and Zhang, Bingjing and
	Wu, Tak-Lon and Ruan, Yang and Ekanayake, Saliya and Hughes, Adam
	and Fox, Geoffrey},
  title = {Hybrid cloud and cluster computing paradigms for life science applications.},
  journal = {BMC Bioinformatics},
  year = {2010},
  volume = {11 Suppl 12},
  pages = {S3},
  abstract = {Clouds and MapReduce have shown themselves to be a broadly useful
	approach to scientific computing especially for parallel data intensive
	applications. However they have limited applicability to some areas
	such as data mining because MapReduce has poor performance on problems
	with an iterative structure present in the linear algebra that underlies
	much data analysis. Such problems can be run efficiently on clusters
	using MPI leading to a hybrid cloud and cluster environment. This
	motivates the design and implementation of an open source Iterative
	MapReduce system Twister.Comparisons of Amazon, Azure, and traditional
	Linux and Windows environments on common applications have shown
	encouraging performance and usability comparisons in several important
	non iterative cases. These are linked to MPI applications for final
	stages of the data analysis. Further we have released the open source
	Twister Iterative MapReduce and benchmarked it against basic MapReduce
	(Hadoop) and MPI in information retrieval and life sciences applications.The
	hybrid cloud (MapReduce) and cluster (MPI) approach offers an attractive
	production environment while Twister promises a uniform programming
	environment for many Life Sciences applications.We used commercial
	clouds Amazon and Azure and the NSF resource FutureGrid to perform
	detailed comparisons and evaluations of different approaches to data
	intensive computing. Several applications were developed in MPI,
	MapReduce and Twister in these different environments.},
  doi = {10.1186/1471-2105-11-S12-S3},
  institution = {School of Informatics and Computing, Indiana University, Bloomington,
	IN 47405, USA. xqiu@indiana.edu},
  keywords = {Biological Science Disciplines; Cluster Analysis; Computational Biology,
	methods; Data Mining; Metagenomics; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-11-S12-S3},
  pmid = {21210982},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S3}
}

@INPROCEEDINGS{Ranger2007,
  author = {Ranger, C. and Raghuraman, R. and Penmetsa, A. and Bradski, G. and
	Kozyrakis, C.},
  title = {Evaluating MapReduce for Multi-core and Multiprocessor Systems},
  booktitle = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th
	International Symposium on},
  year = {2007},
  pages = {13-24},
  month = {Feb},
  doi = {10.1109/HPCA.2007.346181},
  keywords = {fault tolerance;multi-threading;multiprocessing systems;performance
	evaluation;scheduling;MapReduce;Phoenix;data partitioning;distributed
	system;dynamic task scheduling;error recovery;fault tolerance;functional-style
	code;multicore system;multiprocessor system;programming API and;shared-memory
	systems;thread creation;Concurrent computing;Dynamic scheduling;Fault
	tolerance;Laboratories;Multiprocessing systems;Parallel programming;Processor
	scheduling;Programming profession;Runtime;Yarn}
}

@ARTICLE{Schatz2009,
  author = {Schatz, Michael C.},
  title = {CloudBurst: highly sensitive read mapping with MapReduce.},
  journal = {Bioinformatics},
  year = {2009},
  volume = {25},
  pages = {1363--1369},
  number = {11},
  month = {Jun},
  abstract = {Next-generation DNA sequencing machines are generating an enormous
	amount of sequence data, placing unprecedented demands on traditional
	single-processor read-mapping algorithms. CloudBurst is a new parallel
	read-mapping algorithm optimized for mapping next-generation sequence
	data to the human genome and other reference genomes, for use in
	a variety of biological analyses including SNP discovery, genotyping
	and personal genomics. It is modeled after the short read-mapping
	program RMAP, and reports either all alignments or the unambiguous
	best alignment for each read with any number of mismatches or differences.
	This level of sensitivity could be prohibitively time consuming,
	but CloudBurst uses the open-source Hadoop implementation of MapReduce
	to parallelize execution using multiple compute nodes.CloudBurst's
	running time scales linearly with the number of reads mapped, and
	with near linear speedup as the number of processors increases. In
	a 24-processor core configuration, CloudBurst is up to 30 times faster
	than RMAP executing on a single core, while computing an identical
	set of alignments. Using a larger remote compute cloud with 96 cores,
	CloudBurst improved performance by >100-fold, reducing the running
	time from hours to mere minutes for typical jobs involving mapping
	of millions of short reads to the human genome.CloudBurst is available
	open-source as a model for parallelizing algorithms with MapReduce
	at (http://cloudburst-bio.sourceforge.net/).},
  doi = {10.1093/bioinformatics/btp236},
  institution = {Center for Bioinformatics and Computational Biology, University of
	Maryland, College Park, MD 20742, USA. mschatz@umiacs.umd.edu},
  keywords = {Algorithms; Animals; Computational Biology, methods; DNA; Genome;
	Humans; Internet; Sequence Alignment; Sequence Analysis, DNA, methods},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btp236},
  pmid = {19357099},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bioinformatics/btp236}
}

@ARTICLE{Schumacher2014,
  author = {Schumacher, André and Pireddu, Luca and Niemenmaa, Matti and Kallio,
	Aleksi and Korpelainen, Eija and Zanetti, Gianluigi and Heljanko,
	Keijo},
  title = {SeqPig: simple and scalable scripting for large sequencing data sets
	in Hadoop.},
  journal = {Bioinformatics},
  year = {2014},
  volume = {30},
  pages = {119--120},
  number = {1},
  month = {Jan},
  abstract = {Hadoop MapReduce-based approaches have become increasingly popular
	due to their scalability in processing large sequencing datasets.
	However, as these methods typically require in-depth expertise in
	Hadoop and Java, they are still out of reach of many bioinformaticians.
	To solve this problem, we have created SeqPig, a library and a collection
	of tools to manipulate, analyze and query sequencing datasets in
	a scalable and simple manner. SeqPigscripts use the Hadoop-based
	distributed scripting engine Apache Pig, which automatically parallelizes
	and distributes data processing tasks. We demonstrate SeqPig's scalability
	over many computing nodes and illustrate its use with example scripts.Available
	under the open source MIT license at http://sourceforge.net/projects/seqpig/},
  doi = {10.1093/bioinformatics/btt601},
  institution = {Aalto University School of Science and Helsinki Institute for Information
	Technology HIIT, Finland, International Computer Science Institute,
	Berkeley, CA, USA, CRS4-Center for Advanced Studies, Research and
	Development in Sardinia, Italy and CSC-IT Center for Science, Finland.},
  keywords = {High-Throughput Screening Assays, methods; Software Design},
  language = {eng},
  medline-pst = {ppublish},
  owner = {bau04c},
  pii = {btt601},
  pmid = {24149054},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bioinformatics/btt601}
}

@ARTICLE{Taylor2010,
  author = {Taylor, Ronald C.},
  title = {An overview of the Hadoop/MapReduce/HBase framework and its current
	applications in bioinformatics.},
  journal = {BMC Bioinformatics},
  year = {2010},
  volume = {11 Suppl 12},
  pages = {S1},
  abstract = {Bioinformatics researchers are now confronted with analysis of ultra
	large-scale data sets, a problem that will only increase at an alarming
	rate in coming years. Recent developments in open source software,
	that is, the Hadoop project and associated software, provide a foundation
	for scaling to petabyte scale data warehouses on Linux clusters,
	providing fault-tolerant parallelized analysis on such data using
	a programming style named MapReduce.An overview is given of the current
	usage within the bioinformatics community of Hadoop, a top-level
	Apache Software Foundation project, and of associated open source
	software projects. The concepts behind Hadoop and the associated
	HBase project are defined, and current bioinformatics software that
	employ Hadoop is described. The focus is on next-generation sequencing,
	as the leading application area to date.Hadoop and the MapReduce
	programming paradigm already have a substantial base in the bioinformatics
	community, especially in the field of next-generation sequencing
	analysis, and such use is increasing. This is due to the cost-effectiveness
	of Hadoop-based analysis on commodity Linux clusters, and in the
	cloud via data upload to cloud vendors who have implemented Hadoop/HBase;
	and due to the effectiveness and ease-of-use of the MapReduce method
	in parallelization of many data analysis algorithms.},
  doi = {10.1186/1471-2105-11-S12-S1},
  institution = {Computational Biology and Bioinformatics Group, Pacific Northwest
	National Laboratory, Richland, Washington 99352, USA. ronald.taylor@pnl.gov},
  keywords = {Algorithms; Cluster Analysis; Computational Biology, methods; High-Throughput
	Nucleotide Sequencing; Software},
  language = {eng},
  medline-pst = {epublish},
  owner = {bau04c},
  pii = {1471-2105-11-S12-S1},
  pmid = {21210976},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S1}
}

@ARTICLE{Zou2013,
  author = {Zou, Quan and Li, Xu-Bin and Jiang, Wen-Rui and Lin, Zi-Yu and Li,
	Gui-Lin and Chen, Ke},
  title = {Survey of MapReduce frame operation in bioinformatics.},
  journal = {Brief Bioinform},
  year = {2013},
  month = {Feb},
  abstract = {Bioinformatics is challenged by the fact that traditional analysis
	tools have difficulty in processing large-scale data from high-throughput
	sequencing. The open source Apache Hadoop project, which adopts the
	MapReduce framework and a distributed file system, has recently given
	bioinformatics researchers an opportunity to achieve scalable, efficient
	and reliable computing performance on Linux clusters and on cloud
	computing services. In this article, we present MapReduce frame-based
	applications that can be employed in the next-generation sequencing
	and other biological domains. In addition, we discuss the challenges
	faced by this field as well as the future works on parallel computing
	in bioinformatics.},
  doi = {10.1093/bib/bbs088},
  language = {eng},
  medline-pst = {aheadofprint},
  owner = {bau04c},
  pii = {bbs088},
  pmid = {23396756},
  timestamp = {2014.05.01},
  url = {http://dx.doi.org/10.1093/bib/bbs088}
}

@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

